{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuntianshen/Desktop/College/Research/privacy_research/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 94846.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>: O\n",
      "ĠPatient: O\n",
      "ĠJohn: B-PATIENT\n",
      "ĠDoe: L-PATIENT\n",
      "Ġwas: O\n",
      "Ġadmitted: O\n",
      "Ġto: O\n",
      "Ġthe: O\n",
      "Ġhospital: O\n",
      "Ġon: O\n",
      "Ġ12: B-DATE\n",
      "th: I-DATE\n",
      "ĠJuly: I-DATE\n",
      "Ġ2020: L-DATE\n",
      ".: O\n",
      "</s>: O\n"
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "# from huggingface_hub import snapshot_download\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# transformers_model = \"obi/deid_roberta_i2b2\"  # Example model for de-identification\n",
    "\n",
    "# # Download the model snapshot\n",
    "# snapshot_download(repo_id=transformers_model)\n",
    "\n",
    "# # Instantiate tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(transformers_model)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(transformers_model)\n",
    "\n",
    "# # Example text\n",
    "# text = \"Patient John Doe was admitted to the hospital on 12th July 2020.\"\n",
    "\n",
    "# # Tokenize input\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# # Perform inference\n",
    "# outputs = model(**inputs)\n",
    "# logits = outputs.logits\n",
    "# predicted_token_class_ids = logits.argmax(-1).squeeze().tolist()\n",
    "\n",
    "# # Map predicted token class IDs to labels\n",
    "# predicted_labels = [model.config.id2label[id] for id in predicted_token_class_ids]\n",
    "\n",
    "# # Print tokens with corresponding labels\n",
    "# tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "# for token, label in zip(tokens, predicted_labels):\n",
    "#     print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(filepath: str):\n",
    "    return pd.read_json(filepath, orient=\"records\")\n",
    "\n",
    "df = read_file(\"../data/obfuscated_data_06.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERSON', 'PHONE_NUMBER', 'EMAIL', 'URL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:presidio-analyzer:model_to_presidio_entity_mapping is missing from configuration, using default\n",
      "WARNING:presidio-analyzer:low_score_entity_names is missing from configuration, using default\n",
      "WARNING:presidio-analyzer:labels_to_ignore is missing from configuration, using default\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "WARNING:presidio-analyzer:Entity ID doesn't have the corresponding recognizer in language : en\n",
      "WARNING:presidio-analyzer:Entity USERNAME doesn't have the corresponding recognizer in language : en\n",
      "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "WARNING:presidio-analyzer:Found entity ORGANIZATION which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity ORGANIZATION which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity DATE_TIME which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity DATE_TIME which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity ORGANIZATION which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity DATE_TIME which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity DATE_TIME which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity ORGANIZATION which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity ORGANIZATION which is not supported by Presidio\n",
      "WARNING:presidio-analyzer:Found entity DATE_TIME which is not supported by Presidio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following entities:\n",
      "type: ORGANIZATION, start: 178, end: 182, score: 1.0 ---- bank\n",
      "type: ORGANIZATION, start: 2566, end: 2570, score: 1.0 ---- bank\n",
      "type: DATE_TIME, start: 3133, end: 3137, score: 1.0 ---- 2019\n",
      "type: DATE_TIME, start: 3183, end: 3187, score: 1.0 ---- 2019\n",
      "type: ORGANIZATION, start: 2556, end: 2565, score: 0.9800000190734863 ---- Santander\n",
      "type: PERSON, start: 3069, end: 3072, score: 0.9800000190734863 ---- dam\n",
      "type: DATE_TIME, start: 3142, end: 3146, score: 0.9800000190734863 ---- 2019\n",
      "type: PERSON, start: 3073, end: 3082, score: 0.9399999976158142 ---- santander\n",
      "type: ORGANIZATION, start: 168, end: 177, score: 0.9300000071525574 ---- Santander\n",
      "type: DATE_TIME, start: 3188, end: 3190, score: 0.8799999952316284 ---- 50\n",
      "type: ORGANIZATION, start: 4162, end: 4167, score: 0.8600000143051147 ---- great\n",
      "type: PERSON, start: 3047, end: 3056, score: 0.8299999833106995 ---- santander\n",
      "type: PERSON, start: 3087, end: 3089, score: 0.8199999928474426 ---- es\n",
      "type: PERSON, start: 3035, end: 3040, score: 0.7900000214576721 ---- https\n",
      "type: DATE_TIME, start: 3138, end: 3141, score: 0.7900000214576721 ---- ias\n",
      "type: PERSON, start: 3061, end: 3068, score: 0.7099999785423279 ---- content\n",
      "type: PERSON, start: 3043, end: 3046, score: 0.5099999904632568 ---- www\n",
      "type: PERSON, start: 3057, end: 3060, score: 0.4399999976158142 ---- com\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "from presidio_analyzer import (\n",
    "    RecognizerResult,\n",
    "    EntityRecognizer,\n",
    "    AnalysisExplanation,\n",
    ")\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "from configuration import BERT_DEID_CONFIGURATION\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"presidio-analyzer\")\n",
    "\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForTokenClassification,\n",
    "        pipeline,\n",
    "        TokenClassificationPipeline,\n",
    "    )\n",
    "\n",
    "except ImportError:\n",
    "    logger.error(\"transformers is not installed\")\n",
    "\n",
    "\n",
    "class TransformersRecognizer(EntityRecognizer):\n",
    "    \"\"\"\n",
    "    Wrapper for a transformers model, if needed to be used within Presidio Analyzer.\n",
    "    The class loads models hosted on HuggingFace - https://huggingface.co/\n",
    "    and loads the model and tokenizer into a TokenClassification pipeline.\n",
    "    Samples are split into short text chunks, ideally shorter than max_length input_ids of the individual model,\n",
    "    to avoid truncation by the Tokenizer and loss of information\n",
    "\n",
    "    A configuration object should be maintained for each dataset-model combination and translate\n",
    "    entities names into a standardized view. A sample of a configuration file is attached in\n",
    "    the example.\n",
    "    :param supported_entities: List of entities to run inference on\n",
    "    :type supported_entities: Optional[List[str]]\n",
    "    :param pipeline: Instance of a TokenClassificationPipeline including a Tokenizer and a Model, defaults to None\n",
    "    :type pipeline: Optional[TokenClassificationPipeline], optional\n",
    "    :param model_path: string referencing a HuggingFace uploaded model to be used for Inference, defaults to None\n",
    "    :type model_path: Optional[str], optional\n",
    "\n",
    "    :example\n",
    "    >from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "    >model_path = \"obi/deid_roberta_i2b2\"\n",
    "    >transformers_recognizer = TransformersRecognizer(model_path=model_path,\n",
    "    >supported_entities = model_configuration.get(\"PRESIDIO_SUPPORTED_ENTITIES\"))\n",
    "    >transformers_recognizer.load_transformer(**model_configuration)\n",
    "    >registry = RecognizerRegistry()\n",
    "    >registry.add_recognizer(transformers_recognizer)\n",
    "    >analyzer = AnalyzerEngine(registry=registry)\n",
    "    >sample = \"My name is Christopher and I live in Irbid.\"\n",
    "    >results = analyzer.analyze(sample, language=\"en\",return_decision_process=True)\n",
    "\n",
    "    >for result in results:\n",
    "    >    print(result,'----', sample[result.start:result.end])\n",
    "    \"\"\"\n",
    "\n",
    "    def load(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: Optional[str] = None,\n",
    "        pipeline: Optional[TokenClassificationPipeline] = None,\n",
    "        supported_entities: Optional[List[str]] = None,\n",
    "    ):\n",
    "        if not supported_entities:\n",
    "            supported_entities = BERT_DEID_CONFIGURATION[\n",
    "                \"PRESIDIO_SUPPORTED_ENTITIES\"\n",
    "            ]\n",
    "        super().__init__(\n",
    "            supported_entities=supported_entities,\n",
    "            name=f\"Transformers model {model_path}\",\n",
    "        )\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = pipeline\n",
    "        self.is_loaded = False\n",
    "\n",
    "        self.aggregation_mechanism = None\n",
    "        self.ignore_labels = None\n",
    "        self.model_to_presidio_mapping = None\n",
    "        self.entity_mapping = None\n",
    "        self.default_explanation = None\n",
    "        self.text_overlap_length = None\n",
    "        self.chunk_length = None\n",
    "        self.id_entity_name = None\n",
    "        self.id_score_reduction = None\n",
    "\n",
    "    def load_transformer(self, **kwargs) -> None:\n",
    "        \"\"\"Load external configuration parameters and set default values.\n",
    "\n",
    "        :param kwargs: define default values for class attributes and modify pipeline behavior\n",
    "        **DATASET_TO_PRESIDIO_MAPPING (dict) - defines mapping entity strings from dataset format to Presidio format\n",
    "        **MODEL_TO_PRESIDIO_MAPPING (dict) -  defines mapping entity strings from chosen model format to Presidio format\n",
    "        **SUB_WORD_AGGREGATION(str) - define how to aggregate sub-word tokens into full words and spans as defined\n",
    "        in HuggingFace https://huggingface.co/transformers/v4.8.0/main_classes/pipelines.html#transformers.TokenClassificationPipeline # noqa\n",
    "        **CHUNK_OVERLAP_SIZE (int) - number of overlapping characters in each text chunk\n",
    "        when splitting a single text into multiple inferences\n",
    "        **CHUNK_SIZE (int) - number of characters in each chunk of text\n",
    "        **LABELS_TO_IGNORE (List(str)) - List of entities to skip evaluation. Defaults to [\"O\"]\n",
    "        **DEFAULT_EXPLANATION (str) - string format to use for prediction explanations\n",
    "        **ID_ENTITY_NAME (str) - name of the ID entity\n",
    "        **ID_SCORE_REDUCTION (float) - score multiplier for ID entities\n",
    "        \"\"\"\n",
    "\n",
    "        self.entity_mapping = kwargs.get(\"DATASET_TO_PRESIDIO_MAPPING\", {})\n",
    "        self.model_to_presidio_mapping = kwargs.get(\"MODEL_TO_PRESIDIO_MAPPING\", {})\n",
    "        self.ignore_labels = kwargs.get(\"LABELS_TO_IGNORE\", [\"O\"])\n",
    "        self.aggregation_mechanism = kwargs.get(\"SUB_WORD_AGGREGATION\", \"simple\")\n",
    "        self.default_explanation = kwargs.get(\"DEFAULT_EXPLANATION\", None)\n",
    "        self.text_overlap_length = kwargs.get(\"CHUNK_OVERLAP_SIZE\", 40)\n",
    "        self.chunk_length = kwargs.get(\"CHUNK_SIZE\", 600)\n",
    "        self.id_entity_name = kwargs.get(\"ID_ENTITY_NAME\", \"ID\")\n",
    "        self.id_score_reduction = kwargs.get(\"ID_SCORE_REDUCTION\", 0.5)\n",
    "\n",
    "        if not self.pipeline:\n",
    "            if not self.model_path:\n",
    "                self.model_path = \"obi/deid_roberta_i2b2\"\n",
    "                logger.warning(\n",
    "                    f\"Both 'model' and 'model_path' arguments are None. Using default model_path={self.model_path}\"\n",
    "                )\n",
    "\n",
    "        self._load_pipeline()\n",
    "\n",
    "    def _load_pipeline(self) -> None:\n",
    "        \"\"\"Initialize NER transformers pipeline using the model_path provided\"\"\"\n",
    "\n",
    "        logging.debug(f\"Initializing NER pipeline using {self.model_path} path\")\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        self.pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=AutoModelForTokenClassification.from_pretrained(self.model_path),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(self.model_path),\n",
    "            # Will attempt to group sub-entities to word level\n",
    "            aggregation_strategy=self.aggregation_mechanism,\n",
    "            device=device,\n",
    "            framework=\"pt\",\n",
    "            ignore_labels=self.ignore_labels,\n",
    "        )\n",
    "\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def get_supported_entities(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return supported entities by this model.\n",
    "        :return: List of the supported entities.\n",
    "        \"\"\"\n",
    "        return self.supported_entities\n",
    "\n",
    "    # Class to use transformers with Presidio as an external recognizer.\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Analyze text using transformers model to produce NER tagging.\n",
    "        :param text : The text for analysis.\n",
    "        :param entities: Not working properly for this recognizer.\n",
    "        :param nlp_artifacts: Not used by this recognizer.\n",
    "        :return: The list of Presidio RecognizerResult constructed from the recognized\n",
    "            transformers detections.\n",
    "        \"\"\"\n",
    "\n",
    "        results = list()\n",
    "        # Run transformer model on the provided text\n",
    "        ner_results = self._get_ner_results_for_text(text)\n",
    "\n",
    "        for res in ner_results:\n",
    "            res[\"entity_group\"] = self.__check_label_transformer(res[\"entity_group\"])\n",
    "            if not res[\"entity_group\"]:\n",
    "                continue\n",
    "\n",
    "            if res[\"entity_group\"] == self.id_entity_name:\n",
    "                print(f\"ID entity found, multiplying score by {self.id_score_reduction}\")\n",
    "                res[\"score\"] = res[\"score\"] * self.id_score_reduction\n",
    "\n",
    "            textual_explanation = self.default_explanation.format(res[\"entity_group\"])\n",
    "            explanation = self.build_transformers_explanation(\n",
    "                float(round(res[\"score\"], 2)), textual_explanation, res[\"word\"]\n",
    "            )\n",
    "            transformers_result = self._convert_to_recognizer_result(res, explanation)\n",
    "\n",
    "            results.append(transformers_result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def split_text_to_word_chunks(\n",
    "        input_length: int, chunk_length: int, overlap_length: int\n",
    "    ) -> List[List]:\n",
    "        \"\"\"The function calculates chunks of text with size chunk_length. Each chunk has overlap_length number of\n",
    "        words to create context and continuity for the model\n",
    "\n",
    "        :param input_length: Length of input_ids for a given text\n",
    "        :type input_length: int\n",
    "        :param chunk_length: Length of each chunk of input_ids.\n",
    "        Should match the max input length of the transformer model\n",
    "        :type chunk_length: int\n",
    "        :param overlap_length: Number of overlapping words in each chunk\n",
    "        :type overlap_length: int\n",
    "        :return: List of start and end positions for individual text chunks\n",
    "        :rtype: List[List]\n",
    "        \"\"\"\n",
    "        if input_length < chunk_length:\n",
    "            return [[0, input_length]]\n",
    "        if chunk_length <= overlap_length:\n",
    "            logger.warning(\n",
    "                \"overlap_length should be shorter than chunk_length, setting overlap_length to by half of chunk_length\"\n",
    "            )\n",
    "            overlap_length = chunk_length // 2\n",
    "        return [\n",
    "            [i, min([i + chunk_length, input_length])]\n",
    "            for i in range(\n",
    "                0, input_length - overlap_length, chunk_length - overlap_length\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _get_ner_results_for_text(self, text: str) -> List[dict]:\n",
    "        \"\"\"The function runs model inference on the provided text.\n",
    "        The text is split into chunks with n overlapping characters.\n",
    "        The results are then aggregated and duplications are removed.\n",
    "\n",
    "        :param text: The text to run inference on\n",
    "        :type text: str\n",
    "        :return: List of entity predictions on the word level\n",
    "        :rtype: List[dict]\n",
    "        \"\"\"\n",
    "        model_max_length = self.pipeline.tokenizer.model_max_length\n",
    "        # calculate inputs based on the text\n",
    "        text_length = len(text)\n",
    "        # split text into chunks\n",
    "        if text_length <= model_max_length:\n",
    "            predictions = self.pipeline(text)\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"splitting the text into chunks, length {text_length} > {model_max_length}\"\n",
    "            )\n",
    "            predictions = list()\n",
    "            chunk_indexes = TransformersRecognizer.split_text_to_word_chunks(\n",
    "                text_length, self.chunk_length, self.text_overlap_length\n",
    "                )\n",
    "\n",
    "            # iterate over text chunks and run inference\n",
    "            for chunk_start, chunk_end in chunk_indexes:\n",
    "                chunk_text = text[chunk_start:chunk_end]\n",
    "                chunk_preds = self.pipeline(chunk_text)\n",
    "\n",
    "                # align indexes to match the original text - add to each position the value of chunk_start\n",
    "                aligned_predictions = list()\n",
    "                for prediction in chunk_preds:\n",
    "                    prediction_tmp = copy.deepcopy(prediction)\n",
    "                    prediction_tmp[\"start\"] += chunk_start\n",
    "                    prediction_tmp[\"end\"] += chunk_start\n",
    "                    aligned_predictions.append(prediction_tmp)\n",
    "\n",
    "                predictions.extend(aligned_predictions)\n",
    "\n",
    "        # remove duplicates\n",
    "        predictions = [dict(t) for t in {tuple(d.items()) for d in predictions}]\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_recognizer_result(\n",
    "        prediction_result: dict, explanation: AnalysisExplanation\n",
    "    ) -> RecognizerResult:\n",
    "        \"\"\"The method parses NER model predictions into a RecognizerResult format to enable down the stream analysis\n",
    "\n",
    "        :param prediction_result: A single example of entity prediction\n",
    "        :type prediction_result: dict\n",
    "        :param explanation: Textual representation of model prediction\n",
    "        :type explanation: str\n",
    "        :return: An instance of RecognizerResult which is used to model evaluation calculations\n",
    "        :rtype: RecognizerResult\n",
    "        \"\"\"\n",
    "\n",
    "        transformers_results = RecognizerResult(\n",
    "            entity_type=prediction_result[\"entity_group\"],\n",
    "            start=prediction_result[\"start\"],\n",
    "            end=prediction_result[\"end\"],\n",
    "            score=float(round(prediction_result[\"score\"], 2)),\n",
    "            analysis_explanation=explanation,\n",
    "        )\n",
    "\n",
    "        return transformers_results\n",
    "\n",
    "    def build_transformers_explanation(\n",
    "        self,\n",
    "        original_score: float,\n",
    "        explanation: str,\n",
    "        pattern: str,\n",
    "    ) -> AnalysisExplanation:\n",
    "        \"\"\"\n",
    "        Create explanation for why this result was detected.\n",
    "        :param original_score: Score given by this recognizer\n",
    "        :param explanation: Explanation string\n",
    "        :param pattern: Regex pattern used\n",
    "        :return Structured explanation and scores of a NER model prediction\n",
    "        :rtype: AnalysisExplanation\n",
    "        \"\"\"\n",
    "        explanation = AnalysisExplanation(\n",
    "            recognizer=self.__class__.__name__,\n",
    "            original_score=float(original_score),\n",
    "            textual_explanation=explanation,\n",
    "            pattern=pattern,\n",
    "        )\n",
    "        return explanation\n",
    "\n",
    "    def __check_label_transformer(self, label: str) -> Optional[str]:\n",
    "        \"\"\"The function validates the predicted label is identified by Presidio\n",
    "        and maps the string into a Presidio representation\n",
    "        :param label: Predicted label by the model\n",
    "        :return: Returns the adjusted entity name\n",
    "        \"\"\"\n",
    "\n",
    "        # convert model label to presidio label\n",
    "        entity = self.model_to_presidio_mapping.get(label, None)\n",
    "\n",
    "        if entity in self.ignore_labels:\n",
    "            return None\n",
    "\n",
    "        if entity is None:\n",
    "            logger.warning(f\"Found unrecognized label {label}, returning entity as is\")\n",
    "            return label\n",
    "\n",
    "        if entity not in self.supported_entities:\n",
    "            logger.warning(f\"Found entity {entity} which is not supported by Presidio\")\n",
    "            return entity\n",
    "        return entity\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "    from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "    import spacy\n",
    "\n",
    "    model_path = \"obi/deid_roberta_i2b2\"\n",
    "    supported_entities = BERT_DEID_CONFIGURATION.get(\n",
    "        \"PRESIDIO_SUPPORTED_ENTITIES\")\n",
    "    print(supported_entities)\n",
    "    transformers_recognizer = TransformersRecognizer(model_path=model_path,\n",
    "                                                     supported_entities=supported_entities)\n",
    "\n",
    "    # This would download a large (~500Mb) model on the first run\n",
    "    transformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n",
    "\n",
    "    # Add transformers model to the registry\n",
    "    registry = RecognizerRegistry()\n",
    "    registry.add_recognizer(transformers_recognizer)\n",
    "    registry.remove_recognizer(\"SpacyRecognizer\")\n",
    "\n",
    "    # Use small spacy model, for faster inference.\n",
    "    if not spacy.util.is_package(\"en_core_web_trf\"):\n",
    "        spacy.cli.download(\"en_core_web_trf\")\n",
    "\n",
    "    nlp_configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_trf\"}],\n",
    "    }\n",
    "\n",
    "    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n",
    "\n",
    "    analyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)\n",
    "\n",
    "\n",
    "    input_idx = 5\n",
    "    input_text = df.iloc[input_idx].full_text\n",
    "\n",
    "    # input_text = \"My name is John and I live in NY\"\n",
    "    # results = analyzer.analyze(input_text, language=\"en\",\n",
    "    #                            return_decision_process=True,\n",
    "    #                            )\n",
    "    entities = [\"PERSON\", \"EMAIL\", \"URL\", \"PHONE_NUMBER\", \"ID\", \"USERNAME\", \"STREET_ADDRESS\"]\n",
    "\n",
    "    # Analyze the text to find PII\n",
    "    results_analyzed = analyzer.analyze(text=input_text, language=\"en\", entities=entities, \n",
    "                                        score_threshold=None, return_decision_process=True)\n",
    "    print(\"Found the following entities:\")\n",
    "    for result in results_analyzed:\n",
    "        print(result, '----', input_text[result.start:result.end])\n",
    "\n",
    "    # Found the following entities:\n",
    "    # type: PERSON, start: 11, end: 15, score: 1.0 ---- John\n",
    "    # type: LOCATION, start: 30, end: 32, score: 1.0 ---- NY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
