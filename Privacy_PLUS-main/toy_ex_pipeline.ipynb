{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = 'your-openai-key'\n",
    "\n",
    "# Function to generate prompts dynamically\n",
    "def get_prompt(category):\n",
    "    name_keywords = [\"name\", \"teacher\", \"student\", \"friend\", \"husband\", \"child\", \"son\", \"daughter\", \"cousin\", \"sister\", \"brother\", \n",
    "                     \"instructor\", \"colleague\", \"partner\", \"employee\", \"researcher\", \"school\", \"account\", \"resort\"]\n",
    "    for keyword in name_keywords:\n",
    "        if keyword in category.lower():\n",
    "            return f\"Generate a {category.lower()}. Just give me one name with no additional output. Do not output anything like a chatbot, I only need the result without any other unnecessary outputs. Do NOT end your output with a punctuation mark.\"\n",
    "    \n",
    "    # Default prompt for non-name categories\n",
    "    return f\"Generate a {category.lower()}. Just give me one result with no additional output. Do not output anything like a chatbot, I only need the result without any other unnecessary outputs. Do NOT end your output with a punctuation mark.\"\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_synthetic_data(category):\n",
    "    prompt = get_prompt(category)\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Function to replace placeholders and record locations\n",
    "def replace_placeholders(text):\n",
    "    pattern = re.compile(r'<([^>]+)>')\n",
    "    placeholder_locations = []\n",
    "    synthetic_data = []\n",
    "    replaced_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for match in pattern.finditer(text):\n",
    "        placeholder = match.group(1)\n",
    "        placeholder_locations.append((match.start(), match.end(), placeholder))\n",
    "        start = match.start() + offset\n",
    "        end = match.end() + offset\n",
    "        synthetic_value = generate_synthetic_data(placeholder)\n",
    "        \n",
    "        # Record the synthetic PII positions\n",
    "        replaced_text = replaced_text[:start] + synthetic_value + replaced_text[end:]\n",
    "        synthetic_data.append((start, start + len(synthetic_value), placeholder, synthetic_value))\n",
    "        offset += len(synthetic_value) - (end - start)\n",
    "\n",
    "    return replaced_text, placeholder_locations, synthetic_data\n",
    "\n",
    "# Function to highlight PII instances in green\n",
    "def highlight_text(text, locations, is_synthetic=False):\n",
    "    highlighted_text = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    for loc in locations:\n",
    "        if is_synthetic:\n",
    "            start, end, category, value = loc\n",
    "        else:\n",
    "            start, end, category = loc\n",
    "        highlighted_text += text[last_end:start]\n",
    "        highlighted_text += f'\\x1b[6;30;42m{text[start:end]}\\x1b[0m'\n",
    "        last_end = end\n",
    "    \n",
    "    highlighted_text += text[last_end:]\n",
    "    return highlighted_text\n",
    "\n",
    "# Function to highlight original placeholders\n",
    "def highlight_placeholders(text, placeholder_locations):\n",
    "    return highlight_text(text, placeholder_locations)\n",
    "\n",
    "# Function to highlight synthetic PII\n",
    "def highlight_synthetic_pii(text, synthetic_data):\n",
    "    return highlight_text(text, synthetic_data, is_synthetic=True)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"Hi <TEACHER>, please contact <STUDENT> at <EMAIL ADDRESS>. My date of birth is <DOB> and I live at <ADDRESS>. \n",
    "You can call me at <TELEPHONE>. <STUDENT> has an appointment with <TEACHER> tomorrow.\"\"\"\n",
    "\n",
    "replaced_text, placeholder_locations, synthetic_info = replace_placeholders(text)\n",
    "highlighted_original = highlight_placeholders(text, placeholder_locations)\n",
    "highlighted_text = highlight_synthetic_pii(replaced_text, synthetic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder_locations: [(3, 12, 'TEACHER'), (29, 38, 'STUDENT'), (42, 57, 'EMAIL ADDRESS'), (79, 84, 'DOB'), (99, 108, 'ADDRESS'), (130, 141, 'TELEPHONE'), (143, 152, 'STUDENT'), (177, 186, 'TEACHER')]\n",
      "synthetic_info: [(3, 15, 'TEACHER', 'Mr. Thompson'), (32, 38, 'STUDENT', 'Sophia'), (42, 65, 'EMAIL ADDRESS', 'springtime123@email.com'), (87, 103, 'DOB', 'January 15, 1992'), (118, 131, 'ADDRESS', '24 Oak Street'), (153, 167, 'TELEPHONE', '1-855-555-0199'), (169, 174, 'STUDENT', 'Emily'), (199, 211, 'TEACHER', 'Ms. Anderson')]\n",
      "\n",
      "Hi \u001b[6;30;42m<TEACHER>\u001b[0m, please contact \u001b[6;30;42m<STUDENT>\u001b[0m at \u001b[6;30;42m<EMAIL ADDRESS>\u001b[0m. My date of birth is \u001b[6;30;42m<DOB>\u001b[0m and I live at \u001b[6;30;42m<ADDRESS>\u001b[0m. \n",
      "You can call me at \u001b[6;30;42m<TELEPHONE>\u001b[0m. \u001b[6;30;42m<STUDENT>\u001b[0m has an appointment with \u001b[6;30;42m<TEACHER>\u001b[0m tomorrow.\n",
      "-----------------------------------------------------------------------------------------\n",
      "Hi Mr. Thompson, please contact Sophia at springtime123@email.com. My date of birth is January 15, 1992 and I live at 24 Oak Street. \n",
      "You can call me at 1-855-555-0199. Emily has an appointment with Ms. Anderson tomorrow.\n",
      "-----------------------------------------------------------------------------------------\n",
      "Hi \u001b[6;30;42mMr. Thompson\u001b[0m, please contact \u001b[6;30;42mSophia\u001b[0m at \u001b[6;30;42mspringtime123@email.com\u001b[0m. My date of birth is \u001b[6;30;42mJanuary 15, 1992\u001b[0m and I live at \u001b[6;30;42m24 Oak Street\u001b[0m. \n",
      "You can call me at \u001b[6;30;42m1-855-555-0199\u001b[0m. \u001b[6;30;42mEmily\u001b[0m has an appointment with \u001b[6;30;42mMs. Anderson\u001b[0m tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Print original and replaced highlighted texts\n",
    "print(f\"placeholder_locations: {placeholder_locations}\")\n",
    "print(f\"synthetic_info: {synthetic_info}\\n\")\n",
    "print(highlighted_original)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(replaced_text)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: Mr. Thompson ---- Mr. Thompson\n",
      "True: Sophia ---- Sophia\n",
      "True: springtime123@email.com ---- springtime123@email.com\n",
      "True: January 15, 1992 ---- January 15, 1992\n",
      "True: 24 Oak Street ---- 24 Oak Street\n",
      "True: 1-855-555-0199 ---- 1-855-555-0199\n",
      "True: Emily ---- Emily\n",
      "True: Ms. Anderson ---- Ms. Anderson\n"
     ]
    }
   ],
   "source": [
    "def check_PII_position(synthetic_info):\n",
    "    for item in synthetic_info:\n",
    "        start, end, category, value = item\n",
    "        print(f\"{replaced_text[start:end] == value}: {replaced_text[start:end]} ---- {value}\")\n",
    "        \n",
    "check_PII_position(synthetic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example: teacherstudentchat00006.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the processed_transcripts here\n",
    "# with open('original_transcripts.txt', 'r') as f:\n",
    "#     original_transcripts = json.load(f)\n",
    "\n",
    "# # Verify the content\n",
    "# print(original_transcripts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = original_transcripts[4]\n",
    "\n",
    "# replaced_text, placeholder_locations, synthetic_info = replace_placeholders(text)\n",
    "# highlighted_original = highlight_placeholders(text, placeholder_locations)\n",
    "# highlighted_text = highlight_synthetic_pii(replaced_text, synthetic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print placeholder_locations, synthetic_info, highlighted original text, highlighted replaced text\n",
    "# print(f\"placeholder_locations: {placeholder_locations}\")\n",
    "# print(f\"synthetic_info: {synthetic_info}\\n\")\n",
    "# print(highlighted_original)\n",
    "# print(\"-----------------------------------------------------------------------------------------\")\n",
    "# print(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_PII_position(synthetic_info):\n",
    "#     for item in synthetic_info:\n",
    "#         start, end, category, value = item\n",
    "#         print(f\"{replaced_text[start:end] == value}: {replaced_text[start:end]} ---- {value}\")\n",
    "        \n",
    "# check_PII_position(synthetic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presidio Implementation Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider, TransformersNlpEngine, NerModelConfiguration\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n",
    "import names\n",
    "import random\n",
    "from random import randrange\n",
    "from datetime import timedelta, datetime\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Create configuration containing engine name and models\n",
    "def get_configuration(spaCy_model: str):\n",
    "    configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n",
    "                {\"lang_code\": \"en\", \"model_name\": spaCy_model}],\n",
    "    }\n",
    "\n",
    "    return configuration\n",
    "\n",
    "def get_conf_file(spaCy_model: str, transformer_model: str = None):\n",
    "    snapshot_download(repo_id=transformer_model)\n",
    "    # Instantiate to make sure it's downloaded during installation and not runtime\n",
    "    AutoTokenizer.from_pretrained(transformer_model)\n",
    "    AutoModelForTokenClassification.from_pretrained(transformer_model)\n",
    "\n",
    "    config_dict = {\n",
    "        \"en_core_web_lg + obi/deid_roberta_i2b2\": \"Config/lg+roberta.yaml\",\n",
    "        \"en_core_web_lg + StanfordAIMI/stanford-deidentifier-base\": \"Config/lg+stanford.yaml\",\n",
    "        \"en_core_web_trf + obi/deid_roberta_i2b2\": \"Config/trf+roberta.yaml\",\n",
    "        \"en_core_web_trf + StanfordAIMI/stanford-deidentifier-base\": \"Config/trf+stanford.yaml\",\n",
    "    }\n",
    "\n",
    "    # Create configuration containing engine name and models\n",
    "    conf_file = config_dict[spaCy_model + ' + ' + transformer_model]\n",
    "\n",
    "    return conf_file\n",
    "\n",
    "# Function to create NLP engine based on configuration\n",
    "def create_nlp_engine(spaCy_model: str, transformer_model: str = None):\n",
    "    if spaCy_model not in [\"en_core_web_lg\", \"en_core_web_trf\"]:\n",
    "        raise ValueError(\"Input spaCy model is not supported.\")\n",
    "    if transformer_model is not None:\n",
    "        if transformer_model not in [\"obi/deid_roberta_i2b2\", \"StanfordAIMI/stanford-deidentifier-base\"]:\n",
    "            print(transformer_model)\n",
    "            raise ValueError(\"Input transformer model is not supported.\")\n",
    "    \n",
    "    # spaCy model only\n",
    "    if transformer_model is None:\n",
    "        configuration = get_configuration(spaCy_model)\n",
    "        provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "\n",
    "    # spaCy model with transformer\n",
    "    else:\n",
    "        conf_file = get_conf_file(spaCy_model, transformer_model)\n",
    "        provider = NlpEngineProvider(conf_file=conf_file)\n",
    "    \n",
    "    nlp_engine = provider.create_engine()\n",
    "    return nlp_engine\n",
    "\n",
    "# Using only spaCy model\n",
    "nlp_engine_spacy_only = create_nlp_engine(spaCy_model = \"en_core_web_lg\")\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine = nlp_engine_spacy_only, # nlp_engine_spacy_only or nlp_engine_with_transformer\n",
    "    supported_languages=[\"en\", \"es\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tutor's and student's names to different fake names.\n",
    "# !pip install faker\n",
    "from faker import Faker\n",
    "\n",
    "# Create an allow list to exclude words from being identified as PII\n",
    "allow_list = [\n",
    "    \"Today\",\n",
    "    \"today\",\n",
    "    \"Yesterday\",\n",
    "    \"yesterday\",\n",
    "    \"Tomorrow\",\n",
    "    \"tomorrow\"\n",
    "]\n",
    "    \n",
    "def de_identify_pii(text_transcript):\n",
    "    # Initialize the analyzer and anonymizer\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "\n",
    "    # Define date range for generating random dates and generate a random date\n",
    "    d1 = datetime.strptime('1/1/2008 1:30 PM', '%m/%d/%Y %I:%M %p')\n",
    "    d2 = datetime.strptime('1/1/2009 4:50 AM', '%m/%d/%Y %I:%M %p')\n",
    "    random_date = (d1 + timedelta(days=random.randint(0, (d2 - d1).days))).strftime('%m/%d/%Y')\n",
    "\n",
    "    fake = Faker()\n",
    "\n",
    "    # Function to generate a unique fake name\n",
    "    def generate_fake_name(existing_names, first_name):\n",
    "        if first_name:\n",
    "            while True:\n",
    "                fake_name = names.get_first_name()\n",
    "                if fake_name not in existing_names:\n",
    "                    return fake_name\n",
    "        else:\n",
    "            while True:\n",
    "                fake_name = names.get_last_name()\n",
    "                if fake_name not in existing_names:\n",
    "                    return fake_name\n",
    "    \n",
    "    # Function to generate a unique fake email\n",
    "    def generate_fake_email(fake_name):\n",
    "        domains = [\"gmail.com\", \"sina.com\", \"outlook.com\"]\n",
    "        return f\"{fake_name.lower()}@{random.choice(domains)}\"\n",
    "    \n",
    "    # Function to generate a unique fake location\n",
    "    def generate_fake_location():\n",
    "        return fake.city()  # Generate a fake city name using Faker\n",
    "\n",
    "    # Function to generate a unique fake phone number\n",
    "    def generate_fake_phone_number():\n",
    "        return f\"555-{random.randint(100, 999)}-{random.randint(1000, 9999)}\"\n",
    "\n",
    "    # Analyze the text to find PII\n",
    "    results_analyzed = analyzer.analyze(text=text_transcript, language=\"en\", return_decision_process=True, allow_list=allow_list)\n",
    "    \n",
    "    # Modify results_analyzed to include Ms., Mrs., and Mr.\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"PERSON\":\n",
    "            s = result.start\n",
    "            \n",
    "             # Checking if Ms., Mrs., or Mr. comes before the name\n",
    "            if (s-4 >= 0) and (text_transcript[s-4:s] == \"Ms. \" or text_transcript[s-4:s] == \"Mr. \"):\n",
    "                result.start = s-4\n",
    "            elif (s-5 >= 0) and text_transcript[s-5:s] == \"Mrs. \":\n",
    "                result.start = s-5\n",
    "                \n",
    "    # Create a mapping of original names to unique fake names\n",
    "    name_mapping = {}\n",
    "    existing_names = set()\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"PERSON\":\n",
    "            original_name = text_transcript[result.start:result.end]\n",
    "            first_name = True\n",
    "            \n",
    "            if original_name.startswith(\"Ms. \") or original_name.startswith(\"Mr. \") or original_name.startswith(\"Mrs. \"):\n",
    "                first_name = False\n",
    "               \n",
    "            if original_name not in name_mapping:\n",
    "                fake_name = generate_fake_name(existing_names, first_name)\n",
    "                \n",
    "                if first_name:\n",
    "                    name_mapping[original_name] = fake_name\n",
    "                else:\n",
    "                    titles = [\"Ms.\", \"Mr.\", \"Mrs.\"]\n",
    "                    name_mapping[original_name] = random.choice(titles) + \" \" + fake_name\n",
    "                \n",
    "                existing_names.add(fake_name)\n",
    "    \n",
    "    # Email mapping to ensure consistent fake emails\n",
    "    email_mapping = {}\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"EMAIL_ADDRESS\":\n",
    "            original_email = text_transcript[result.start:result.end]\n",
    "            if original_email not in email_mapping:\n",
    "                fake_name = generate_fake_name(existing_names, True)\n",
    "                fake_email = generate_fake_email(fake_name)\n",
    "                email_mapping[original_email] = fake_email\n",
    "    \n",
    "    # Phone number mapping to ensure consistent fake phone numbers\n",
    "    phone_mapping = {}\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"PHONE_NUMBER\":\n",
    "            original_phone = text_transcript[result.start:result.end]\n",
    "            if original_phone not in phone_mapping:\n",
    "                fake_phone = generate_fake_phone_number()\n",
    "                phone_mapping[original_phone] = fake_phone\n",
    "\n",
    "    operators = {\n",
    "        \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": lambda text : name_mapping.get(text, text)}),\n",
    "        \"DATE_TIME\": OperatorConfig(\"replace\", {\"new_value\": random_date}),\n",
    "        # Add more categories\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda text: email_mapping.get(text, text)}),\n",
    "        \"LOCATION\": OperatorConfig(\"replace\", {\"new_value\": generate_fake_location()}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": lambda text: phone_mapping.get(text, text)})\n",
    "    }\n",
    "\n",
    "    # Anonymize the text\n",
    "    results_anonymized = anonymizer.anonymize(\n",
    "        text=text_transcript,\n",
    "        analyzer_results=results_analyzed,\n",
    "        operators=operators\n",
    "    )\n",
    "\n",
    "    return results_analyzed, results_anonymized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_space(s, i):\n",
    "    return s[:i] + ' ' + s[i:]\n",
    "\n",
    "def remove_possible_url(s):\n",
    "    spaces = []\n",
    "    for i in range(len(s)-1):\n",
    "        if i > 1 and s[i-2:i] == '...' and s[i+1].isalpha():\n",
    "            spaces.append(i+1)\n",
    "\n",
    "    for space in reversed(spaces):  # reversed to not mess up indices\n",
    "        s = insert_space(s, space)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Mr. Thompson, please contact Sophia at springtime123@email.com. My date of birth is January 15, 1992 and I live at 24 Oak Street. \n",
      "You can call me at 1-855-555-0199. Emily has an appointment with Ms. Anderson tomorrow.\n",
      "-----------------------------------------------------------------------------------------\n",
      "Hi Mr. Thompson, please contact Sophia at springtime123@email.com. My date of birth is January 15, 1992 and I live at 24 Oak Street. \n",
      "You can call me at 1-855-555-0199. Emily has an appointment with Ms. Anderson tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Print text before and after remove_possible_url()\n",
    "print(replaced_text)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "replaced_text2 = remove_possible_url(replaced_text)\n",
    "print(replaced_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_analyzed, results_anonymized = de_identify_pii(replaced_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: EMAIL_ADDRESS, start: 42, end: 65, score: 1.0, type: PERSON, start: 3, end: 15, score: 0.85, type: PERSON, start: 32, end: 38, score: 0.85, type: DATE_TIME, start: 87, end: 103, score: 0.85, type: PERSON, start: 169, end: 174, score: 0.85, type: PERSON, start: 199, end: 211, score: 0.85, type: URL, start: 56, end: 65, score: 0.5, type: PHONE_NUMBER, start: 153, end: 167, score: 0.4, type: IN_PAN, start: 153, end: 163, score: 0.05]\n",
      "type: EMAIL_ADDRESS, start: 42, end: 65, score: 1.0\n",
      "type: PERSON, start: 3, end: 15, score: 0.85\n",
      "type: PERSON, start: 32, end: 38, score: 0.85\n",
      "type: DATE_TIME, start: 87, end: 103, score: 0.85\n",
      "type: PERSON, start: 169, end: 174, score: 0.85\n",
      "type: PERSON, start: 199, end: 211, score: 0.85\n",
      "type: URL, start: 56, end: 65, score: 0.5\n",
      "type: PHONE_NUMBER, start: 153, end: 167, score: 0.4\n",
      "type: IN_PAN, start: 153, end: 163, score: 0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "text: Hi Ms. Brannen, please contact Kara at deborah@sina.com. My date of birth is 03/28/2008 and I live at 24 Oak Street. \n",
      "You can call me at 555-372-8963. Donna has an appointment with Mr. Montgomery tomorrow.\n",
      "items:\n",
      "[\n",
      "    {'start': 181, 'end': 195, 'entity_type': 'PERSON', 'text': 'Mr. Montgomery', 'operator': 'custom'},\n",
      "    {'start': 151, 'end': 156, 'entity_type': 'PERSON', 'text': 'Donna', 'operator': 'custom'},\n",
      "    {'start': 137, 'end': 149, 'entity_type': 'PHONE_NUMBER', 'text': '555-372-8963', 'operator': 'custom'},\n",
      "    {'start': 77, 'end': 87, 'entity_type': 'DATE_TIME', 'text': '03/28/2008', 'operator': 'replace'},\n",
      "    {'start': 39, 'end': 55, 'entity_type': 'EMAIL_ADDRESS', 'text': 'deborah@sina.com', 'operator': 'custom'},\n",
      "    {'start': 31, 'end': 35, 'entity_type': 'PERSON', 'text': 'Kara', 'operator': 'custom'},\n",
      "    {'start': 3, 'end': 14, 'entity_type': 'PERSON', 'text': 'Ms. Brannen', 'operator': 'custom'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results_analyzed and results_anonymized\n",
    "# results_analyzed is a list\n",
    "print(results_analyzed)\n",
    "for res in results_analyzed:\n",
    "    print(res)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(results_anonymized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII: springtime123@email.com ---- start: 42 ---- end: 65 ---- type: EMAIL_ADDRESS\n",
      "PII: Mr. Thompson ---- start: 3 ---- end: 15 ---- type: PERSON\n",
      "PII: Sophia ---- start: 32 ---- end: 38 ---- type: PERSON\n",
      "PII: January 15, 1992 ---- start: 87 ---- end: 103 ---- type: DATE_TIME\n",
      "PII: Emily ---- start: 169 ---- end: 174 ---- type: PERSON\n",
      "PII: Ms. Anderson ---- start: 199 ---- end: 211 ---- type: PERSON\n",
      "PII: email.com ---- start: 56 ---- end: 65 ---- type: URL\n",
      "PII: 1-855-555-0199 ---- start: 153 ---- end: 167 ---- type: PHONE_NUMBER\n",
      "PII: 1-855-555- ---- start: 153 ---- end: 163 ---- type: IN_PAN\n"
     ]
    }
   ],
   "source": [
    "# Print Presidio-identifed PII from results_analyzed.\n",
    "# Notice email.com is separately detected.\n",
    "# TODO: How to deal with overlapping identified PII? Should we just remove the smaller one from results_analyzed?\n",
    "\n",
    "# Current solution: remove overlapping entities only from the extracted positions of results_analyzed but not from results_analyzed itself\n",
    "# Relevent code (two code chunks below):\n",
    "# positions_analyzed = extract_positions(results_analyzed)\n",
    "# positions_analyzed = remove_overlapping_entities(positions_analyzed)\n",
    "for res in results_analyzed:\n",
    "    print(f\"PII: {replaced_text[res.start:res.end]} ---- start: {res.start} ---- end: {res.end} ---- type: {res.entity_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove overlapping entities\n",
    "def remove_overlapping_entities(entities):\n",
    "    # Sort by start position and length (descending)\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    filtered_entities = []\n",
    "    last_end = -1\n",
    "    \n",
    "    for start, end, entity_type in entities:\n",
    "        # Remove entities that overlap with previously accepted entities\n",
    "        if start >= last_end:\n",
    "            filtered_entities.append((start, end, entity_type))\n",
    "            last_end = end\n",
    "        else:\n",
    "            # Check if the overlapping entity is of higher priority\n",
    "            if filtered_entities and start < filtered_entities[-1][1]:\n",
    "                if end - start > filtered_entities[-1][1] - filtered_entities[-1][0]:\n",
    "                    filtered_entities[-1] = (start, end, entity_type)\n",
    "                elif end - start == filtered_entities[-1][1] - filtered_entities[-1][0]:\n",
    "                    if entity_type == \"EMAIL_ADDRESS\" and filtered_entities[-1][2] == \"URL\":\n",
    "                        filtered_entities[-1] = (start, end, entity_type)\n",
    "\n",
    "    return filtered_entities\n",
    "\n",
    "# Function to highlight text\n",
    "def highlight_text(text, locations):\n",
    "    highlighted_text = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    # for start, end, _ in sorted(locations, key=lambda x: x[0]):\n",
    "    for start, end, _ in locations:\n",
    "        highlighted_text += text[last_end:start]\n",
    "        highlighted_text += f'\\x1b[6;30;42m{text[start:end]}\\x1b[0m'\n",
    "        last_end = end\n",
    "    \n",
    "    highlighted_text += text[last_end:]\n",
    "    return highlighted_text\n",
    "\n",
    "# Extract positions and types from Presidio results\n",
    "def extract_positions(results):\n",
    "    positions = []\n",
    "    for res in results:\n",
    "        positions.append((res.start, res.end, res.entity_type))\n",
    "    return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions_analyzed (before removing overlaps):\n",
      "PII: springtime123@email.com ---- start: 42 ---- end: 65 ---- type: EMAIL_ADDRESS\n",
      "PII: Mr. Thompson ---- start: 3 ---- end: 15 ---- type: PERSON\n",
      "PII: Sophia ---- start: 32 ---- end: 38 ---- type: PERSON\n",
      "PII: January 15, 1992 ---- start: 87 ---- end: 103 ---- type: DATE_TIME\n",
      "PII: Emily ---- start: 169 ---- end: 174 ---- type: PERSON\n",
      "PII: Ms. Anderson ---- start: 199 ---- end: 211 ---- type: PERSON\n",
      "PII: email.com ---- start: 56 ---- end: 65 ---- type: URL\n",
      "PII: 1-855-555-0199 ---- start: 153 ---- end: 167 ---- type: PHONE_NUMBER\n",
      "PII: 1-855-555- ---- start: 153 ---- end: 163 ---- type: IN_PAN\n",
      "-----------------------------------------------------------------------------------------\n",
      "positions_analyzed (after removing overlaps) and sorted:\n",
      "PII: Mr. Thompson ---- start: 3 ---- end: 15 ---- type: PERSON\n",
      "PII: Sophia ---- start: 32 ---- end: 38 ---- type: PERSON\n",
      "PII: springtime123@email.com ---- start: 42 ---- end: 65 ---- type: EMAIL_ADDRESS\n",
      "PII: January 15, 1992 ---- start: 87 ---- end: 103 ---- type: DATE_TIME\n",
      "PII: 1-855-555-0199 ---- start: 153 ---- end: 167 ---- type: PHONE_NUMBER\n",
      "PII: Emily ---- start: 169 ---- end: 174 ---- type: PERSON\n",
      "PII: Ms. Anderson ---- start: 199 ---- end: 211 ---- type: PERSON\n"
     ]
    }
   ],
   "source": [
    "# Extract positions for highlighting\n",
    "positions_analyzed = extract_positions(results_analyzed)\n",
    "positions_anonymized = extract_positions(results_anonymized.items)\n",
    "\n",
    "print(\"positions_analyzed (before removing overlaps):\")\n",
    "for pos in positions_analyzed:\n",
    "    print(f\"PII: {replaced_text[pos[0]:pos[1]]} ---- start: {pos[0]} ---- end: {pos[1]} ---- type: {pos[2]}\")\n",
    "\n",
    "# Remove overlapping entities from the positions\n",
    "positions_analyzed = remove_overlapping_entities(positions_analyzed)\n",
    "positions_anonymized = remove_overlapping_entities(positions_anonymized)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"positions_analyzed (after removing overlaps) and sorted:\")\n",
    "for pos in positions_analyzed:\n",
    "    print(f\"PII: {replaced_text[pos[0]:pos[1]]} ---- start: {pos[0]} ---- end: {pos[1]} ---- type: {pos[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlighted True Text:\n",
      "Hi \u001b[6;30;42mMr. Thompson\u001b[0m, please contact \u001b[6;30;42mSophia\u001b[0m at \u001b[6;30;42mspringtime123@email.com\u001b[0m. My date of birth is \u001b[6;30;42mJanuary 15, 1992\u001b[0m and I live at \u001b[6;30;42m24 Oak Street\u001b[0m. \n",
      "You can call me at \u001b[6;30;42m1-855-555-0199\u001b[0m. \u001b[6;30;42mEmily\u001b[0m has an appointment with \u001b[6;30;42mMs. Anderson\u001b[0m tomorrow.\n",
      "-----------------------------------------------------------------------------------------\n",
      "Highlighted Presidio Identified PIIs:\n",
      "Hi \u001b[6;30;42mMr. Thompson\u001b[0m, please contact \u001b[6;30;42mSophia\u001b[0m at \u001b[6;30;42mspringtime123@email.com\u001b[0m. My date of birth is \u001b[6;30;42mJanuary 15, 1992\u001b[0m and I live at 24 Oak Street. \n",
      "You can call me at \u001b[6;30;42m1-855-555-0199\u001b[0m. \u001b[6;30;42mEmily\u001b[0m has an appointment with \u001b[6;30;42mMs. Anderson\u001b[0m tomorrow.\n",
      "-----------------------------------------------------------------------------------------\n",
      "Highlighted Anonymized Text (Not a focus for RQ1):\n",
      "Hi \u001b[6;30;42mMs. Brannen\u001b[0m, please contact \u001b[6;30;42mKara\u001b[0m at \u001b[6;30;42mdeborah@sina.com\u001b[0m. My date of birth is \u001b[6;30;42m03/28/2008\u001b[0m and I live at 24 Oak Street. \n",
      "You can call me at \u001b[6;30;42m555-372-8963\u001b[0m. \u001b[6;30;42mDonna\u001b[0m has an appointment with \u001b[6;30;42mMr. Montgomery\u001b[0m tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Highlighting the text\n",
    "highlighted_analyzed_text = highlight_text(replaced_text, positions_analyzed)\n",
    "highlighted_anonymized_text = highlight_text(results_anonymized.text, positions_anonymized)\n",
    "\n",
    "print(\"Highlighted True Text:\")\n",
    "print(highlighted_text)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"Highlighted Presidio Identified PIIs:\")\n",
    "print(highlighted_analyzed_text)\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"Highlighted Anonymized Text (Not a focus for RQ1):\")\n",
    "print(highlighted_anonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can already see that Presidio cannot detect the address 29 Acacia Road in this example, which should be closely examined. Presidio supposed to be capable of identifying such address.\n",
    "\n",
    "We can also see that Presidio only detects 'Rodriguez' instead of 'Ms. Rodriguez'. Is \"Ms.\" considered as PII? Does it leak the gender? Should we say that Presidio successfully detects this PII? Same thing for 'Jacobs' and 'Mr. Jacobs'. Notice this is the concern in Step 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from here:\n",
    "1. (Completed) Run Presidio (the default model) on toy example + one example transcript\n",
    "2. (Completed) Get the output start and end indices of Presidio-identified PII (from results_analyzed)\n",
    "3. (Completed) Highlight them as long as the original replaced_text (see above code chunks for highlighting and checking)\n",
    "4. (Completed) Upload the updated code to GitHub main branch by **Monday night (EST)** and send a message in our group Slack channel.\n",
    "5. (Almost Completed) Compare the two start and end indices from original result and presidio-identified result by printing them out and eyeballing them.\n",
    "6. (Completed) By **Tuesday Night (EST)**: Calculate the TP, FP, FN, TN, and thus Recall (R), Precision (P), and F1 Score using **exact index matching** (both start and end indices have to be exactly the same in order to be considered as True Positive). - **Aim to see these results at our next internal group meeting on Wednesday**.\n",
    "7. Run more TSCC transcripts and check whether GPT-3.5 generates some weird outputs for \\<Categories\\> by checking *value* of *synthetic_info* (the fourth item of *synthetic_info*: *synthetic_info[3]*).\n",
    "8. Examine more closely the start and end indices from both results - Consider whether this scenario happens: True PII: \"Sam Altman\", Presidio-detected: \"Sam\" and \"Altman\" separately, or only \"Sam\", or only \"Altman\".\n",
    "9. Think about how to deal with this situation if it happens and think about whether tokenization is necessary. (Can imply double check mechanism: Combine “\\<NAME\\> \\<NAME\\>”, notice there’s only one space in between)\n",
    "10. Calculate the TP, FP, FN, TN, and thus Recall (R), Precision (P), and F1 Score again.\n",
    "11. Connect Presidio with other Transformers and make sure code runs successfully.\n",
    "\n",
    "### Task for 7.3 ~ 7.10\n",
    "- Look at why address is not detected by Presidio (Step 5, eyeballing issues) - OL\n",
    "- Mr. and Ms. issue; Explore on tokenization (inside or outside Presidio) (Step **8 & 9** & 10) - AL\n",
    "- Try connecting Presidio with other Transformers (Step 11) - YW, JS\n",
    "- PII insertion on TSCC dataset - Everyone if you have time, not top priority for this week (will be important for next week).\n",
    "\n",
    "### Future Plans:\n",
    "1. 6.28 ~ 7.5: Steps 1 to 6 + PII insertion\n",
    "2. 7.6 ~ 7.12: Steps 7 ~ 9 + PII insertion + Different LLMs\n",
    "3. 7.13 ~ 7.19: Run on all examples + Different LLMs\n",
    "4. 7.19 ~ 7.26: Different LLMs and get the results from TSCC & MOOC (through API) datasets\n",
    "5. **By 8.2: Finish RQ1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: PIIInfo(start=87, end=103)\n",
      "TP: PIIInfo(start=153, end=167)\n",
      "TP: PIIInfo(start=169, end=174)\n",
      "TP: PIIInfo(start=199, end=211)\n",
      "TP: PIIInfo(start=32, end=38)\n",
      "TP: PIIInfo(start=3, end=15)\n",
      "TP: PIIInfo(start=42, end=65)\n",
      "FN: PIIInfo(start=118, end=131)\n",
      "True Positives: 7\n",
      "False Positives: 0\n",
      "False Negatives: 1\n",
      "Precision: 1.0\n",
      "Recall: 0.875\n",
      "F1 Score: 0.9333333333333333\n",
      "synthetic_info: [(3, 15, 'TEACHER', 'Mr. Thompson'), (32, 38, 'STUDENT', 'Sophia'), (42, 65, 'EMAIL ADDRESS', 'springtime123@email.com'), (87, 103, 'DOB', 'January 15, 1992'), (118, 131, 'ADDRESS', '24 Oak Street'), (153, 167, 'TELEPHONE', '1-855-555-0199'), (169, 174, 'STUDENT', 'Emily'), (199, 211, 'TEACHER', 'Ms. Anderson')]\n",
      "\n",
      "[(3, 15, 'PERSON'), (32, 38, 'PERSON'), (42, 65, 'EMAIL_ADDRESS'), (87, 103, 'DATE_TIME'), (153, 167, 'PHONE_NUMBER'), (169, 174, 'PERSON'), (199, 211, 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Step 6 Completed\n",
    "from collections import namedtuple\n",
    "\n",
    "# Define a  data structure to hold PII information\n",
    "PIIInfo = namedtuple('PIIInfo', ['start', 'end'])\n",
    "\n",
    "def calculate_metrics(synthetic_positions, analyzed_positions):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    synthetic_set = set(synthetic_positions)\n",
    "    analyzed_set = set(analyzed_positions)\n",
    "    \n",
    "    for analyzed in analyzed_set:\n",
    "        if analyzed in synthetic_set:\n",
    "            tp += 1 # true PII, detected by presidio\n",
    "            print(f\"TP: {analyzed}\")\n",
    "        else: # false PII, detected by presidio\n",
    "            fp += 1\n",
    "            print(f\"FP: {analyzed}\")\n",
    "    \n",
    "    for synthetic in synthetic_set:\n",
    "        if synthetic not in analyzed_set:\n",
    "            fn += 1\n",
    "            print(f\"FN: {synthetic}\")\n",
    "    \n",
    "    # TN is not usually calculated in NER tasks, as it would require a clear definition of all non-PII text regions,\n",
    "    # which can be complex. But if needed, it would be the length of the text minus all TP, FP, and FN regions.\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "def compute_precision_recall_f1(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def extract_synthetic_positions(results):\n",
    "    positions = [PIIInfo(start, end) for start, end, _, _ in results]\n",
    "    return positions\n",
    "\n",
    "# Extract positions and types from Presidio results\n",
    "def extract_only_positions(results):\n",
    "    positions = [PIIInfo(start, end) for start, end,c in results]\n",
    "    return positions\n",
    "\n",
    "#positions_analyzed = extract_positions(results_analyzed)\n",
    "analyzed_positions = extract_only_positions(positions_analyzed)\n",
    "synthetic_positions = extract_synthetic_positions(synthetic_info)\n",
    "# Calculate TP, FP, FN, TN\n",
    "tp, fp, fn = calculate_metrics(synthetic_positions, analyzed_positions)\n",
    "\n",
    "# Compute Precision, Recall, and F1 Score\n",
    "precision, recall, f1 = compute_precision_recall_f1(tp, fp, fn)\n",
    "\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(f\"synthetic_info: {synthetic_info}\\n\")\n",
    "print(positions_analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
