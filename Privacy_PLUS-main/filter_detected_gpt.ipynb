{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main file that need help with.\n",
    "# Input -> Presidio -> Detected Entities -> GPT-4o -> Metric calculations\n",
    "# Only do WITHIN context filteration (need to extract the sentence where the detected entity belongs to and feed to GPT).\n",
    "# No need to do out-of-context filteration\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "# Load the API key from input/secret.json\n",
    "# secret_json = read_json_file('input/secret.json')\n",
    "with open('input/secret.json', 'r', encoding='UTF-8') as file:\n",
    "    secret_json = json.load(file)\n",
    "api_key = secret_json['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def read_file(filepath: str):\n",
    "    return pd.read_json(filepath, orient=\"records\")\n",
    "\n",
    "df = read_file(\"data/obfuscated_data_06.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON\n",
      "['T', 'T', 'F', 'F', 'T', 'T']\n",
      "\n",
      "\n",
      "EMAIL_ADDRESS\n",
      "['Of course, please provide the pairs for me to evaluate.']\n",
      "\n",
      "\n",
      "URL\n",
      "['T', 'T']\n",
      "\n",
      "\n",
      "PHONE_NUMBER\n",
      "['Sure, please provide the pairs you would like me to evaluate.']\n",
      "\n",
      "\n",
      "[(5, 'https://www.santander.com/content/dam/santander-com/es/documentos/informe-anual-de-sostenibilidad/2019/ias-2019-informe-de-', 'URL', (3035, 3158)), (5, 'https://www.greatplacetowork.com/resources/blog/why-is-diversity-inclusion-in-the-workplace-important', 'URL', (4150, 4251))]\n",
      "\n",
      "\n",
      "Filtered entities: [(0, 'Angela Meyer', 'PERSON', (1039, 1051)), (7, 'Nathalie Sylla', 'PERSON', (52, 66)), (7, 'Nathalie Sylla', 'PERSON', (2281, 2295)), (7, 'Nathalie Sylla', 'PERSON', (3648, 3662)), (5, 'https://www.santander.com/content/dam/santander-com/es/documentos/informe-anual-de-sostenibilidad/2019/ias-2019-informe-de-', 'URL', (3035, 3158)), (5, 'https://www.greatplacetowork.com/resources/blog/why-is-diversity-inclusion-in-the-workplace-important', 'URL', (4150, 4251))]\n"
     ]
    }
   ],
   "source": [
    "# Be aware of the generated cost.\n",
    "\n",
    "def check_pii_entities_gpt4o(detected_entities):\n",
    "    # Separate entities by category\n",
    "    person_entities = [entity for entity in detected_entities if entity[2] == 'PERSON']\n",
    "    email_entities = [entity for entity in detected_entities if entity[2] == 'EMAIL_ADDRESS']\n",
    "    url_entities = [entity for entity in detected_entities if entity[2] == 'URL']\n",
    "    phone_entities = [entity for entity in detected_entities if entity[2] == 'PHONE_NUMBER']\n",
    "\n",
    "    # Function to extract the sentence that the PII appears in\n",
    "    def extract_sentence(text, s, e):\n",
    "        # Find the start of the sentence\n",
    "        sentence_start = s\n",
    "        while sentence_start > 0 and text[sentence_start - 1] not in '.!?':\n",
    "            sentence_start -= 1\n",
    "        \n",
    "        # Find the end of the sentence\n",
    "        sentence_end = e\n",
    "        while sentence_end < len(text) and text[sentence_end] not in '.!?':\n",
    "            sentence_end += 1\n",
    "        \n",
    "        # Extract and return the sentence\n",
    "        return text[sentence_start:sentence_end + 1].strip()\n",
    "\n",
    "\n",
    "    # Function to call GPT-4o and check for PII\n",
    "    def check_entities_with_gpt(entities, category, prompt_template):\n",
    "        # Prepare input text for GPT by batching entities\n",
    "        prompt = prompt_template + \"\\n\\n\"\n",
    "        for entity in entities:\n",
    "            idx, entity_text, _, positions = entity\n",
    "            (s,e) = positions\n",
    "            sentence = extract_sentence(df.iloc[idx].full_text, s, e)\n",
    "            \n",
    "            prompt += f\"({entity_text}, {sentence})\\n\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        gpt_results = response.choices[0].message.content.strip().split('\\n')\n",
    "\n",
    "        print(category)\n",
    "        print(gpt_results)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Process the response and keep entities marked as True (PII detected)\n",
    "        filtered_entities = [\n",
    "            entity for entity, result in zip(entities, gpt_results) if result.strip().upper() == 'T'\n",
    "        ]\n",
    "        \n",
    "        return filtered_entities\n",
    "\n",
    "    # Define prompt templates for each category\n",
    "    person_prompt = \"For each of the following pairs, please decide if the first value is a student's name in the context of the second value (return just a list of 'T's for Trues and 'F's for Falses):\"\n",
    "    email_prompt = \"For each of the following pairs, please decide if the first value is a personal email address in the context of the second value (return just a list of 'T's for Trues and 'F's for Falses):\"\n",
    "    url_prompt = \"For each of the following pairs, please decide if the first value is a personal URL in the context of the second value (return just a list of 'T's for Trues and 'F's for Falses):\"\n",
    "    phone_prompt = \"For each of the following pairs, please decide if the first value is a personal phone number in the context of the second value (return just a list of 'T's for Trues and 'F's for Falses):\"\n",
    "\n",
    "    # Process each category with GPT-4o\n",
    "    valid_person_entities = check_entities_with_gpt(person_entities, 'PERSON', person_prompt)\n",
    "    valid_email_entities = check_entities_with_gpt(email_entities, 'EMAIL_ADDRESS', email_prompt)\n",
    "    valid_url_entities = check_entities_with_gpt(url_entities, 'URL', url_prompt)\n",
    "    valid_phone_entities = check_entities_with_gpt(phone_entities, 'PHONE_NUMBER', phone_prompt)\n",
    "\n",
    "    print(valid_url_entities)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Combine all valid entities into a single list\n",
    "    valid_entities = valid_person_entities + valid_email_entities + valid_url_entities + valid_phone_entities\n",
    "    \n",
    "    # TODO: Might need to sort based on file_idx (first item in tuple) before return.\n",
    "    return valid_entities\n",
    "\n",
    "\n",
    "detected_entities = []\n",
    "# with open('output/pii_detected_trf_filtered.txt', 'r') as file:\n",
    "#     for line in file:\n",
    "#         entity = ast.literal_eval(line.strip())\n",
    "#         detected_entities.append(entity)\n",
    "\n",
    "with open('output/pii_detected_trf_filtered.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            entity = ast.literal_eval(line.strip())\n",
    "            idx, _, _, _ = entity\n",
    "            if idx < 8: # Start from a few examples to try\n",
    "                detected_entities.append(entity)\n",
    "\n",
    "# Call the function to filter valid entities\n",
    "filtered_entities = check_pii_entities_gpt4o(detected_entities)\n",
    "print(f\"Filtered entities: {filtered_entities}\")\n",
    "\n",
    "# Save the filtered out entities in 'output/pii_detected_gpt.txt' or any suitable file name.\n",
    "with open('output/pii_detected_gpt.txt', 'w') as f:\n",
    "    for entity in filtered_entities:\n",
    "        f.write(f\"{entity}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22688, 5)\n",
      "['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Example of loading a text file\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(filepath: str):\n",
    "    return pd.read_json(filepath, orient=\"records\")\n",
    "\n",
    "df = read_file(\"data/obfuscated_data_06.json\")\n",
    "print(df.shape)\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: Visualization (Visual Thinking) – Module 1\n",
      "\n",
      "Challenge & Selection:\n",
      "\n",
      "The tool that I have used and selected for challenge is Visualization that I feel plays the major role in  the entire process of design thinking tools. I work in a creative domain where the bulk of my work  depends on the visual representation of any product or approach or business deal. We work on logos,  presentation, graphic designs, illustration etc., To outline the challenge, we’ve started getting more of  negative surveys ratings on the output which we have shared in the recent days. Then I called for a  stakeholder meeting and wanted to use the visual thinking approach to discuss regarding the  challenge and to come up with different solutions. To get those solutions, I use this approach to  make everyone to contribute and think outside box and come up with a new suggestions based on  the data which is available and apart from the data what are other human centric reasons for the dip  in the survey. To elaborate more I used spider diagrams and posters to reflect the obstacles that  prompted, which induced participants in the room to think and share more suggestions.\n",
      "\n",
      "Application:\n",
      "\n",
      "I use the spider diagrams/flow charts to logically represent the whole process. And use a logo  representation for each idea that is created with the existing data which is available. To differentiate  the ideas and group them into different verticals, I will use a white board and a colored markers. If I  couldn’t find relevant data details for me to initiate a specific discussion about the consultants daily  routine that gives us the survey and what made them give that. I’d probably have an individual  discussion with the stakeholders to get more information about it as they are operate on the ground  to offer us a deeper insight of the task.\n",
      "\n",
      "If required, I will use tableau - visualized presentation that is very interactive with loads of questions  to be answered related to the concept. Tableau is used extensively because data can be analyzed  very quickly with it.Visualizations are also provided as dashboards and worksheets. Tableau helps us  to create dashboards that provides actionable insights and drive business forward. Tableau gives  fantastic visualizations, in-depth insights and user friendly approach.\n",
      "\n",
      "I called all the stake holders to the conference room for a meeting and projected the spider diagram  and a few presentations of the idea and gave them the a white chart or paper to get draw or project  their thoughts on these concepts while holding people (Staffs working with us), process and  customers in mind on why the survey was dipping. Post the session all charts have been collated and  classified according to the three criteria given. And represented those ideas and established the best  solutions for each criteria and begun to implement them\n",
      "\n",
      "Insight:\n",
      "\n",
      "The learning from this whole exercise is that I have received many solutions or completely different  point of view from the stakeholders as everyone started thinking out of their comfort zone and came  up with a new and a fresh thoughts. This tool of making them visually think had given a whole lot of  new perspective and it was a very productive meeting where everyone could contribute in the  meeting. Which gave them more faith in the process and information in depth. We were able to  touch base and get feedback on the everyday life of consultants and their project requirements,  timeline of the project, visual enhancement of the project, communication in getting their work done  and other areas in terms of customer service.\n",
      "\n",
      "Approach:\n",
      "\n",
      "Next time, I’d give them a brief over the issue before the meeting and post getting their thought  process a visual representation meeting would give them a better output compared to what I have in  this whole process. I will include ethnography method along with this tool to understand the logics  behind every suggestions they have proposed. Also, post getting their thought process on the three  different criteria another meeting should be conducted to understand their logics and reasons behind  those suggestions/ideas.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of loading a text file (Continued)\n",
    "file_idx = 22599\n",
    "input_text = df.iloc[file_idx].full_text\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you can record the cost of provide a reasonable estimate of the cost generated from GPT, that'll be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you finish, please go to evaluator.ipynb and run  calculate_metrics('output/pii_detected_gpt.txt')\n",
    "# and go to evaluator_categories.ipynb and run  evaluate_detected_entities('output/pii_detected_gpt.txt')\n",
    "# The two lines of code are marked with TODO."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
