{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(filepath: str):\n",
    "    return pd.read_json(filepath, orient=\"records\")\n",
    "\n",
    "df = read_file(\"obfuscated_data_06.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22688, 5)\n",
      "['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(list(df.columns))\n",
    "# print(df.iloc[80].full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presidio Implementation Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install presidio_analyzer\n",
    "# !pip3 install presidio_anonymizer\n",
    "# !pip3 install names\n",
    "# !pip3 install names transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider, TransformersNlpEngine, NerModelConfiguration\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n",
    "import names\n",
    "import random\n",
    "from random import randrange\n",
    "from datetime import timedelta, datetime\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Create configuration containing engine name and models\n",
    "def get_configuration(spaCy_model: str):\n",
    "    configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n",
    "                {\"lang_code\": \"en\", \"model_name\": spaCy_model}],\n",
    "    }\n",
    "\n",
    "    return configuration\n",
    "\n",
    "def get_conf_file(spaCy_model: str, transformer_model: str = None):\n",
    "    snapshot_download(repo_id=transformer_model)\n",
    "    # Instantiate to make sure it's downloaded during installation and not runtime\n",
    "    AutoTokenizer.from_pretrained(transformer_model)\n",
    "    AutoModelForTokenClassification.from_pretrained(transformer_model)\n",
    "\n",
    "    config_dict = {\n",
    "        \"en_core_web_lg + obi/deid_roberta_i2b2\": \"Config/lg+roberta.yaml\",\n",
    "        \"en_core_web_lg + StanfordAIMI/stanford-deidentifier-base\": \"Config/lg+stanford.yaml\",\n",
    "        \"en_core_web_trf + obi/deid_roberta_i2b2\": \"Config/trf+roberta.yaml\",\n",
    "        \"en_core_web_trf + StanfordAIMI/stanford-deidentifier-base\": \"Config/trf+stanford.yaml\",\n",
    "    }\n",
    "\n",
    "    # Create configuration containing engine name and models\n",
    "    conf_file = config_dict[spaCy_model + ' + ' + transformer_model]\n",
    "\n",
    "    return conf_file\n",
    "\n",
    "# Function to create NLP engine based on configuration\n",
    "def create_nlp_engine(spaCy_model: str, transformer_model: str = None):\n",
    "    if spaCy_model not in [\"en_core_web_lg\", \"en_core_web_trf\"]:\n",
    "        raise ValueError(\"Input spaCy model is not supported.\")\n",
    "    if transformer_model is not None:\n",
    "        if transformer_model not in [\"obi/deid_roberta_i2b2\", \"StanfordAIMI/stanford-deidentifier-base\"]:\n",
    "            print(transformer_model)\n",
    "            raise ValueError(\"Input transformer model is not supported.\")\n",
    "    \n",
    "    # spaCy model only\n",
    "    if transformer_model is None:\n",
    "        configuration = get_configuration(spaCy_model)\n",
    "        provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "\n",
    "    # spaCy model with transformer\n",
    "    else:\n",
    "        conf_file = get_conf_file(spaCy_model, transformer_model)\n",
    "        provider = NlpEngineProvider(conf_file=conf_file)\n",
    "    \n",
    "    nlp_engine = provider.create_engine()\n",
    "    return nlp_engine\n",
    "\n",
    "# Using only spaCy model\n",
    "nlp_engine_spacy_only = create_nlp_engine(spaCy_model = \"en_core_web_lg\")\n",
    "\n",
    "# Using spaCy model with an additional transformer model\n",
    "# nlp_engine_with_transformer = create_nlp_engine(spaCy_model = \"en_core_web_lg\",\n",
    "#                                                 transformer_model = \"StanfordAIMI/stanford-deidentifier-base\")\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    # nlp_engine_spacy_only or nlp_engine_with_transformer\n",
    "    nlp_engine = nlp_engine_spacy_only,\n",
    "    # nlp_engine = nlp_engine_with_transformer,\n",
    "    supported_languages=[\"en\", \"es\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tutor's and student's names to different fake names.\n",
    "# !pip3 install faker\n",
    "from faker import Faker\n",
    "\n",
    "# Create an allow list to exclude words from being identified as PII\n",
    "# allow_list = [\n",
    "#     \"Today\",\n",
    "#     \"today\",\n",
    "#     \"Yesterday\",\n",
    "#     \"yesterday\",\n",
    "#     \"Tomorrow\",\n",
    "#     \"tomorrow\"\n",
    "# ]\n",
    "    \n",
    "def de_identify_pii(text_transcript):\n",
    "    # Initialize the analyzer and anonymizer\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "\n",
    "    # Define date range for generating random dates and generate a random date\n",
    "    d1 = datetime.strptime('1/1/2008 1:30 PM', '%m/%d/%Y %I:%M %p')\n",
    "    d2 = datetime.strptime('1/1/2009 4:50 AM', '%m/%d/%Y %I:%M %p')\n",
    "    random_date = (d1 + timedelta(days=random.randint(0, (d2 - d1).days))).strftime('%m/%d/%Y')\n",
    "\n",
    "    fake = Faker()\n",
    "\n",
    "    # Function to generate a unique fake name\n",
    "    def generate_fake_name(existing_names):\n",
    "        while True:\n",
    "            fake_name = names.get_first_name()\n",
    "            if fake_name not in existing_names:\n",
    "                return fake_name\n",
    "    \n",
    "    # Function to generate a unique fake email\n",
    "    def generate_fake_email(fake_name):\n",
    "        domains = [\"gmail.com\", \"sina.com\", \"outlook.com\"]\n",
    "        return f\"{fake_name.lower()}@{random.choice(domains)}\"\n",
    "    \n",
    "    # Function to generate a unique fake location\n",
    "    def generate_fake_location():\n",
    "        return fake.city()  # Generate a fake city name using Faker\n",
    "\n",
    "    # Function to generate a unique fake phone number\n",
    "    def generate_fake_phone_number():\n",
    "        return f\"555-{random.randint(100, 999)}-{random.randint(1000, 9999)}\"\n",
    "    \n",
    "    # define entities that you want Presidio to detect\n",
    "    entities = [\"PERSON\", \"EMAIL_ADDRESS\", \"URL\", \"PHONE_NUMBER\", \"LOCATION\"] # Age, gender ? DATE_TIME deleted\n",
    "\n",
    "    # Analyze the text to find PII\n",
    "    results_analyzed = analyzer.analyze(text=text_transcript, entities=entities, language=\"en\", \n",
    "                                        return_decision_process=True, allow_list=None)\n",
    "    \n",
    "    # Create a mapping of original names to unique fake names\n",
    "    name_mapping = {}\n",
    "    existing_names = set()\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"PERSON\":\n",
    "            original_name = text_transcript[result.start:result.end]\n",
    "            if original_name not in name_mapping:\n",
    "                fake_name = generate_fake_name(existing_names)\n",
    "                name_mapping[original_name] = fake_name\n",
    "                existing_names.add(fake_name)\n",
    "\n",
    "    # Email mapping to ensure consistent fake emails\n",
    "    email_mapping = {}\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"EMAIL_ADDRESS\":\n",
    "            original_email = text_transcript[result.start:result.end]\n",
    "            if original_email not in email_mapping:\n",
    "                fake_name = generate_fake_name(existing_names)\n",
    "                fake_email = generate_fake_email(fake_name)\n",
    "                email_mapping[original_email] = fake_email\n",
    "    \n",
    "    # Phone number mapping to ensure consistent fake phone numbers\n",
    "    phone_mapping = {}\n",
    "    for result in results_analyzed:\n",
    "        if result.entity_type == \"PHONE_NUMBER\":\n",
    "            original_phone = text_transcript[result.start:result.end]\n",
    "            if original_phone not in phone_mapping:\n",
    "                fake_phone = generate_fake_phone_number()\n",
    "                phone_mapping[original_phone] = fake_phone\n",
    "\n",
    "    operators = {\n",
    "        \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": lambda text : name_mapping.get(text, text)}),\n",
    "        \"DATE_TIME\": OperatorConfig(\"replace\", {\"new_value\": random_date}),\n",
    "        # Add more categories\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda text: email_mapping.get(text, text)}),\n",
    "        \"LOCATION\": OperatorConfig(\"replace\", {\"new_value\": generate_fake_location()}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": lambda text: phone_mapping.get(text, text)})\n",
    "    }\n",
    "\n",
    "    # Anonymize the text\n",
    "    results_anonymized = anonymizer.anonymize(\n",
    "        text=text_transcript,\n",
    "        analyzer_results=results_analyzed,\n",
    "        operators=operators\n",
    "    )\n",
    "\n",
    "    return results_analyzed, results_anonymized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random eyeballing some transcripts\n",
    "# 15616, 13965, 20229, 4351, 19110 (ID_NUM), 21988 (ID_NUM)\n",
    "# input_idx = 21988\n",
    "# input_text = df.iloc[input_idx].full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_analyzed, results_anonymized = de_identify_pii(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results_analyzed and results_anonymized\n",
    "# results_analyzed is a list\n",
    "# print(results_analyzed)\n",
    "# for res in results_analyzed:\n",
    "#     print(res)\n",
    "# print(\"-----------------------------------------------------------------------------------------\")\n",
    "# print(results_anonymized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in results_analyzed:\n",
    "#     print(f\"PII: {input_text[res.start:res.end]} ---- start: {res.start} ---- end: {res.end} ---- type: {res.entity_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Tuple\n",
    "\n",
    "# # Define the type alias for PII entity\n",
    "# type pii_entity = Tuple[int, str, str, Tuple[int, int]]\n",
    "\n",
    "# def remove_overlapping_entities(entities: List[pii_entity]) -> List[pii_entity]:\n",
    "#     # Sort by essay index, start position, and length (descending)\n",
    "#     entities = sorted(entities, key=lambda x: (x[0], x[3][0], -(x[3][1] - x[3][0])))\n",
    "#     filtered_entities: List[pii_entity] = []\n",
    "#     last_end = -1\n",
    "#     last_index = -1\n",
    "\n",
    "#     for entity in entities:\n",
    "#         input_idx, entity_text, entity_type, (start, end) = entity\n",
    "        \n",
    "#         # Remove entities that overlap with previously accepted entities in the same essay\n",
    "#         if input_idx != last_index or start >= last_end:\n",
    "#             filtered_entities.append(entity)\n",
    "#             last_end = end\n",
    "#             last_index = input_idx\n",
    "#         else:\n",
    "#             # Check if the overlapping entity is of higher priority\n",
    "#             if filtered_entities and start < filtered_entities[-1][3][1]:\n",
    "#                 if end - start > filtered_entities[-1][3][1] - filtered_entities[-1][3][0]:\n",
    "#                     filtered_entities[-1] = entity\n",
    "#                 elif end - start == filtered_entities[-1][3][1] - filtered_entities[-1][3][0]:\n",
    "#                     if entity_type == \"EMAIL_ADDRESS\" and filtered_entities[-1][2] == \"URL\":\n",
    "#                         filtered_entities[-1] = entity\n",
    "\n",
    "#     return filtered_entities\n",
    "\n",
    "# # Function to highlight text\n",
    "# # def highlight_text(text, locations):\n",
    "# #     highlighted_text = \"\"\n",
    "# #     last_end = 0\n",
    "    \n",
    "# #     # for start, end, _ in sorted(locations, key=lambda x: x[0]):\n",
    "# #     for start, end, _ in locations:\n",
    "# #         highlighted_text += text[last_end:start]\n",
    "# #         highlighted_text += f'\\x1b[6;30;42m{text[start:end]}\\x1b[0m'\n",
    "# #         last_end = end\n",
    "    \n",
    "# #     highlighted_text += text[last_end:]\n",
    "# #     return highlighted_text\n",
    "\n",
    "# # Extract positions and types from Presidio results\n",
    "# # def extract_positions(results):\n",
    "# #     positions = []\n",
    "# #     for res in results:\n",
    "# #         # positions.append((res.start, res.end, res.entity_type))\n",
    "# #         positions.append((input_idx, input_text[res.start:res.end], res.entity_type, (res.start, res.end)))\n",
    "# #     return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions_analyzed (before removing overlaps):\n",
      "(21988, 'Anisa Hussain - 8743214\\n\\nNon-Business Use\\n\\nVisualization\\n\\nChallenge & Selection', 'PERSON', (33, 112))\n",
      "(21988, 'David Gray’s', 'PERSON', (1356, 1368))\n",
      "(21988, '30 minutes', 'DATE_TIME', (2454, 2464))\n",
      "(21988, 'Anisa Hussain - 8743214\\n\\nNon-Business Use\\n\\nInsight & Approach', 'PERSON', (3130, 3191))\n",
      "4\n",
      "-----------------------------------------------------------------------------------------\n",
      "positions_analyzed (after removing overlaps) and sorted:\n",
      "(21988, 'Anisa Hussain - 8743214\\n\\nNon-Business Use\\n\\nVisualization\\n\\nChallenge & Selection', 'PERSON', (33, 112))\n",
      "(21988, 'David Gray’s', 'PERSON', (1356, 1368))\n",
      "(21988, '30 minutes', 'DATE_TIME', (2454, 2464))\n",
      "(21988, 'Anisa Hussain - 8743214\\n\\nNon-Business Use\\n\\nInsight & Approach', 'PERSON', (3130, 3191))\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# # Extract positions for highlighting\n",
    "# positions_analyzed = extract_positions(results_analyzed)\n",
    "# positions_anonymized = extract_positions(results_anonymized.items)\n",
    "\n",
    "# print(\"positions_analyzed (before removing overlaps):\")\n",
    "# for pos in positions_analyzed:\n",
    "#     # print(f\"PII: {input_text[pos[0]:pos[1]]} ---- start: {pos[0]} ---- end: {pos[1]} ---- type: {pos[2]}\")\n",
    "#     print(pos)\n",
    "# print(len(positions_analyzed))\n",
    "# # Remove overlapping entities from the positions\n",
    "# positions_analyzed = remove_overlapping_entities(positions_analyzed)\n",
    "# positions_anonymized = remove_overlapping_entities(positions_anonymized)\n",
    "\n",
    "# print(\"-----------------------------------------------------------------------------------------\")\n",
    "# print(\"positions_analyzed (after removing overlaps) and sorted:\")\n",
    "# for pos in positions_analyzed:\n",
    "#     # print(f\"PII: {input_text[pos[0]:pos[1]]} ---- start: {pos[0]} ---- end: {pos[1]} ---- type: {pos[2]}\")\n",
    "#     print(pos)\n",
    "# print(len(positions_analyzed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precessing Row 0 ----------\n",
      "Precessing Row 1 ----------\n",
      "Precessing Row 2 ----------\n",
      "Precessing Row 3 ----------\n",
      "Precessing Row 4 ----------\n",
      "Precessing Row 5 ----------\n",
      "Precessing Row 6 ----------\n",
      "Precessing Row 7 ----------\n",
      "Precessing Row 8 ----------\n",
      "Precessing Row 9 ----------\n",
      "Precessing Row 10 ----------\n",
      "Precessing Row 11 ----------\n",
      "Precessing Row 12 ----------\n",
      "Precessing Row 13 ----------\n",
      "Precessing Row 14 ----------\n",
      "Precessing Row 15 ----------\n",
      "Precessing Row 16 ----------\n",
      "Precessing Row 17 ----------\n",
      "Precessing Row 18 ----------\n",
      "Precessing Row 19 ----------\n",
      "Precessing Row 20 ----------\n",
      "Precessing Row 21 ----------\n",
      "Precessing Row 22 ----------\n",
      "Precessing Row 23 ----------\n",
      "Precessing Row 24 ----------\n",
      "Precessing Row 25 ----------\n",
      "Precessing Row 26 ----------\n",
      "Precessing Row 27 ----------\n",
      "Precessing Row 28 ----------\n",
      "Precessing Row 29 ----------\n",
      "Precessing Row 30 ----------\n",
      "Precessing Row 31 ----------\n",
      "Precessing Row 32 ----------\n",
      "Precessing Row 33 ----------\n",
      "Precessing Row 34 ----------\n",
      "Precessing Row 35 ----------\n",
      "Precessing Row 36 ----------\n",
      "Precessing Row 37 ----------\n",
      "Precessing Row 38 ----------\n",
      "Precessing Row 39 ----------\n",
      "Precessing Row 40 ----------\n",
      "Precessing Row 41 ----------\n",
      "Precessing Row 42 ----------\n",
      "Precessing Row 43 ----------\n",
      "Precessing Row 44 ----------\n",
      "Precessing Row 45 ----------\n",
      "Precessing Row 46 ----------\n",
      "Precessing Row 47 ----------\n",
      "Precessing Row 48 ----------\n",
      "Precessing Row 49 ----------\n",
      "Precessing Row 50 ----------\n",
      "Precessing Row 51 ----------\n",
      "Precessing Row 52 ----------\n",
      "Precessing Row 53 ----------\n",
      "Precessing Row 54 ----------\n",
      "Precessing Row 55 ----------\n",
      "Precessing Row 56 ----------\n",
      "Precessing Row 57 ----------\n",
      "Precessing Row 58 ----------\n",
      "Precessing Row 59 ----------\n",
      "Precessing Row 60 ----------\n",
      "Precessing Row 61 ----------\n",
      "Precessing Row 62 ----------\n",
      "Precessing Row 63 ----------\n",
      "Precessing Row 64 ----------\n",
      "Precessing Row 65 ----------\n",
      "Precessing Row 66 ----------\n",
      "Precessing Row 67 ----------\n",
      "Precessing Row 68 ----------\n",
      "Precessing Row 69 ----------\n",
      "Precessing Row 70 ----------\n",
      "Precessing Row 71 ----------\n",
      "Precessing Row 72 ----------\n",
      "Precessing Row 73 ----------\n",
      "Precessing Row 74 ----------\n",
      "Precessing Row 75 ----------\n",
      "Precessing Row 76 ----------\n",
      "Precessing Row 77 ----------\n",
      "Precessing Row 78 ----------\n",
      "Precessing Row 79 ----------\n",
      "Precessing Row 80 ----------\n",
      "Precessing Row 81 ----------\n",
      "Precessing Row 82 ----------\n",
      "Precessing Row 83 ----------\n",
      "Precessing Row 84 ----------\n",
      "Precessing Row 85 ----------\n",
      "Precessing Row 86 ----------\n",
      "Precessing Row 87 ----------\n",
      "Precessing Row 88 ----------\n",
      "Precessing Row 89 ----------\n",
      "Precessing Row 90 ----------\n",
      "Precessing Row 91 ----------\n",
      "Precessing Row 92 ----------\n",
      "Precessing Row 93 ----------\n",
      "Precessing Row 94 ----------\n",
      "Precessing Row 95 ----------\n",
      "Precessing Row 96 ----------\n",
      "Precessing Row 97 ----------\n",
      "Precessing Row 98 ----------\n",
      "Precessing Row 99 ----------\n",
      "Precessing Row 100 ----------\n",
      "Precessing Row 101 ----------\n",
      "Precessing Row 102 ----------\n",
      "Precessing Row 103 ----------\n",
      "Precessing Row 104 ----------\n",
      "Precessing Row 105 ----------\n",
      "Precessing Row 106 ----------\n",
      "Precessing Row 107 ----------\n",
      "Precessing Row 108 ----------\n",
      "Precessing Row 109 ----------\n",
      "Precessing Row 110 ----------\n",
      "Precessing Row 111 ----------\n",
      "Precessing Row 112 ----------\n",
      "Precessing Row 113 ----------\n",
      "Precessing Row 114 ----------\n",
      "Precessing Row 115 ----------\n",
      "Precessing Row 116 ----------\n",
      "Precessing Row 117 ----------\n",
      "Precessing Row 118 ----------\n",
      "Precessing Row 119 ----------\n",
      "Precessing Row 120 ----------\n",
      "Precessing Row 121 ----------\n",
      "Precessing Row 122 ----------\n",
      "Precessing Row 123 ----------\n",
      "Precessing Row 124 ----------\n",
      "Precessing Row 125 ----------\n",
      "Precessing Row 126 ----------\n",
      "Precessing Row 127 ----------\n",
      "Precessing Row 128 ----------\n",
      "Precessing Row 129 ----------\n",
      "Precessing Row 130 ----------\n",
      "Precessing Row 131 ----------\n",
      "Precessing Row 132 ----------\n",
      "Precessing Row 133 ----------\n",
      "Precessing Row 134 ----------\n",
      "Precessing Row 135 ----------\n",
      "Precessing Row 136 ----------\n",
      "Precessing Row 137 ----------\n",
      "Precessing Row 138 ----------\n",
      "Precessing Row 139 ----------\n",
      "Precessing Row 140 ----------\n",
      "Precessing Row 141 ----------\n",
      "Precessing Row 142 ----------\n",
      "Precessing Row 143 ----------\n",
      "Precessing Row 144 ----------\n",
      "Precessing Row 145 ----------\n",
      "Precessing Row 146 ----------\n",
      "Precessing Row 147 ----------\n",
      "Precessing Row 148 ----------\n",
      "Precessing Row 149 ----------\n",
      "Precessing Row 150 ----------\n",
      "Precessing Row 151 ----------\n",
      "Precessing Row 152 ----------\n",
      "Precessing Row 153 ----------\n",
      "Precessing Row 154 ----------\n",
      "Precessing Row 155 ----------\n",
      "Precessing Row 156 ----------\n",
      "Precessing Row 157 ----------\n",
      "Precessing Row 158 ----------\n",
      "Precessing Row 159 ----------\n",
      "Precessing Row 160 ----------\n",
      "Precessing Row 161 ----------\n",
      "Precessing Row 162 ----------\n",
      "Precessing Row 163 ----------\n",
      "Precessing Row 164 ----------\n",
      "Precessing Row 165 ----------\n",
      "Precessing Row 166 ----------\n",
      "Precessing Row 167 ----------\n",
      "Precessing Row 168 ----------\n",
      "Precessing Row 169 ----------\n",
      "Precessing Row 170 ----------\n",
      "Precessing Row 171 ----------\n",
      "Precessing Row 172 ----------\n",
      "Precessing Row 173 ----------\n",
      "Precessing Row 174 ----------\n",
      "Precessing Row 175 ----------\n",
      "Precessing Row 176 ----------\n",
      "Precessing Row 177 ----------\n",
      "Precessing Row 178 ----------\n",
      "Precessing Row 179 ----------\n",
      "Precessing Row 180 ----------\n",
      "Precessing Row 181 ----------\n",
      "Precessing Row 182 ----------\n",
      "Precessing Row 183 ----------\n",
      "Precessing Row 184 ----------\n",
      "Precessing Row 185 ----------\n",
      "Precessing Row 186 ----------\n",
      "Precessing Row 187 ----------\n",
      "Precessing Row 188 ----------\n",
      "Precessing Row 189 ----------\n",
      "Precessing Row 190 ----------\n",
      "Precessing Row 191 ----------\n",
      "Precessing Row 192 ----------\n",
      "Precessing Row 193 ----------\n",
      "Precessing Row 194 ----------\n",
      "Precessing Row 195 ----------\n",
      "Precessing Row 196 ----------\n",
      "Precessing Row 197 ----------\n",
      "Precessing Row 198 ----------\n",
      "Precessing Row 199 ----------\n",
      "Precessing Row 200 ----------\n",
      "Precessing Row 201 ----------\n",
      "Precessing Row 202 ----------\n",
      "Precessing Row 203 ----------\n",
      "Precessing Row 204 ----------\n",
      "Precessing Row 205 ----------\n",
      "Precessing Row 206 ----------\n",
      "Precessing Row 207 ----------\n",
      "Precessing Row 208 ----------\n",
      "Precessing Row 209 ----------\n",
      "Precessing Row 210 ----------\n",
      "Precessing Row 211 ----------\n",
      "Precessing Row 212 ----------\n",
      "Precessing Row 213 ----------\n",
      "Precessing Row 214 ----------\n",
      "Precessing Row 215 ----------\n",
      "Precessing Row 216 ----------\n",
      "Precessing Row 217 ----------\n",
      "Precessing Row 218 ----------\n",
      "Precessing Row 219 ----------\n",
      "Precessing Row 220 ----------\n",
      "Precessing Row 221 ----------\n",
      "Precessing Row 222 ----------\n",
      "Precessing Row 223 ----------\n",
      "Precessing Row 224 ----------\n",
      "Precessing Row 225 ----------\n",
      "Precessing Row 226 ----------\n",
      "Precessing Row 227 ----------\n",
      "Precessing Row 228 ----------\n",
      "Precessing Row 229 ----------\n",
      "Precessing Row 230 ----------\n",
      "Precessing Row 231 ----------\n",
      "Precessing Row 232 ----------\n",
      "Precessing Row 233 ----------\n",
      "Precessing Row 234 ----------\n",
      "Precessing Row 235 ----------\n",
      "Precessing Row 236 ----------\n",
      "Precessing Row 237 ----------\n",
      "Precessing Row 238 ----------\n",
      "Precessing Row 239 ----------\n",
      "Precessing Row 240 ----------\n",
      "Precessing Row 241 ----------\n",
      "Precessing Row 242 ----------\n",
      "Precessing Row 243 ----------\n",
      "Precessing Row 244 ----------\n",
      "Precessing Row 245 ----------\n",
      "Precessing Row 246 ----------\n",
      "Precessing Row 247 ----------\n",
      "Precessing Row 248 ----------\n",
      "Precessing Row 249 ----------\n",
      "Precessing Row 250 ----------\n",
      "Precessing Row 251 ----------\n",
      "Precessing Row 252 ----------\n",
      "Precessing Row 253 ----------\n",
      "Precessing Row 254 ----------\n",
      "Precessing Row 255 ----------\n",
      "Precessing Row 256 ----------\n",
      "Precessing Row 257 ----------\n",
      "Precessing Row 258 ----------\n",
      "Precessing Row 259 ----------\n",
      "Precessing Row 260 ----------\n",
      "Precessing Row 261 ----------\n",
      "Precessing Row 262 ----------\n",
      "Precessing Row 263 ----------\n",
      "Precessing Row 264 ----------\n",
      "Precessing Row 265 ----------\n",
      "Precessing Row 266 ----------\n",
      "Precessing Row 267 ----------\n",
      "Precessing Row 268 ----------\n",
      "Precessing Row 269 ----------\n",
      "Precessing Row 270 ----------\n",
      "Precessing Row 271 ----------\n",
      "Precessing Row 272 ----------\n",
      "Precessing Row 273 ----------\n",
      "Precessing Row 274 ----------\n",
      "Precessing Row 275 ----------\n",
      "Precessing Row 276 ----------\n",
      "Precessing Row 277 ----------\n",
      "Precessing Row 278 ----------\n",
      "Precessing Row 279 ----------\n",
      "Precessing Row 280 ----------\n",
      "Precessing Row 281 ----------\n",
      "Precessing Row 282 ----------\n",
      "Precessing Row 283 ----------\n",
      "Precessing Row 284 ----------\n",
      "Precessing Row 285 ----------\n",
      "Precessing Row 286 ----------\n",
      "Precessing Row 287 ----------\n",
      "Precessing Row 288 ----------\n",
      "Precessing Row 289 ----------\n",
      "Precessing Row 290 ----------\n",
      "Precessing Row 291 ----------\n",
      "Precessing Row 292 ----------\n",
      "Precessing Row 293 ----------\n",
      "Precessing Row 294 ----------\n",
      "Precessing Row 295 ----------\n",
      "Precessing Row 296 ----------\n",
      "Precessing Row 297 ----------\n",
      "Precessing Row 298 ----------\n",
      "Precessing Row 299 ----------\n",
      "Precessing Row 300 ----------\n",
      "Precessing Row 301 ----------\n",
      "Precessing Row 302 ----------\n",
      "Precessing Row 303 ----------\n",
      "Precessing Row 304 ----------\n",
      "Precessing Row 305 ----------\n",
      "Precessing Row 306 ----------\n",
      "Precessing Row 307 ----------\n",
      "Precessing Row 308 ----------\n",
      "Precessing Row 309 ----------\n",
      "Precessing Row 310 ----------\n",
      "Precessing Row 311 ----------\n",
      "Precessing Row 312 ----------\n",
      "Precessing Row 313 ----------\n",
      "Precessing Row 314 ----------\n",
      "Precessing Row 315 ----------\n",
      "Precessing Row 316 ----------\n",
      "Precessing Row 317 ----------\n",
      "Precessing Row 318 ----------\n",
      "Precessing Row 319 ----------\n",
      "Precessing Row 320 ----------\n",
      "Precessing Row 321 ----------\n",
      "Precessing Row 322 ----------\n",
      "Precessing Row 323 ----------\n",
      "Precessing Row 324 ----------\n",
      "Precessing Row 325 ----------\n",
      "Precessing Row 326 ----------\n",
      "Precessing Row 327 ----------\n",
      "Precessing Row 328 ----------\n",
      "Precessing Row 329 ----------\n",
      "Precessing Row 330 ----------\n",
      "Precessing Row 331 ----------\n",
      "Precessing Row 332 ----------\n",
      "Precessing Row 333 ----------\n",
      "Precessing Row 334 ----------\n",
      "Precessing Row 335 ----------\n",
      "Precessing Row 336 ----------\n",
      "Precessing Row 337 ----------\n",
      "Precessing Row 338 ----------\n",
      "Precessing Row 339 ----------\n",
      "Precessing Row 340 ----------\n",
      "Precessing Row 341 ----------\n",
      "Precessing Row 342 ----------\n",
      "Precessing Row 343 ----------\n",
      "Precessing Row 344 ----------\n",
      "Precessing Row 345 ----------\n",
      "Precessing Row 346 ----------\n",
      "Precessing Row 347 ----------\n",
      "Precessing Row 348 ----------\n",
      "Precessing Row 349 ----------\n",
      "Precessing Row 350 ----------\n",
      "Precessing Row 351 ----------\n",
      "Precessing Row 352 ----------\n",
      "Precessing Row 353 ----------\n",
      "Precessing Row 354 ----------\n",
      "Precessing Row 355 ----------\n",
      "Precessing Row 356 ----------\n",
      "Precessing Row 357 ----------\n",
      "Precessing Row 358 ----------\n",
      "Precessing Row 359 ----------\n",
      "Precessing Row 360 ----------\n",
      "Precessing Row 361 ----------\n",
      "Precessing Row 362 ----------\n",
      "Precessing Row 363 ----------\n",
      "Precessing Row 364 ----------\n",
      "Precessing Row 365 ----------\n",
      "Precessing Row 366 ----------\n",
      "Precessing Row 367 ----------\n",
      "Precessing Row 368 ----------\n",
      "Precessing Row 369 ----------\n",
      "Precessing Row 370 ----------\n",
      "Precessing Row 371 ----------\n",
      "Precessing Row 372 ----------\n",
      "Precessing Row 373 ----------\n",
      "Precessing Row 374 ----------\n",
      "Precessing Row 375 ----------\n",
      "Precessing Row 376 ----------\n",
      "Precessing Row 377 ----------\n",
      "Precessing Row 378 ----------\n",
      "Precessing Row 379 ----------\n",
      "Precessing Row 380 ----------\n",
      "Precessing Row 381 ----------\n",
      "Precessing Row 382 ----------\n",
      "Precessing Row 383 ----------\n",
      "Precessing Row 384 ----------\n",
      "Precessing Row 385 ----------\n",
      "Precessing Row 386 ----------\n",
      "Precessing Row 387 ----------\n",
      "Precessing Row 388 ----------\n",
      "Precessing Row 389 ----------\n",
      "Precessing Row 390 ----------\n",
      "Precessing Row 391 ----------\n",
      "Precessing Row 392 ----------\n",
      "Precessing Row 393 ----------\n",
      "Precessing Row 394 ----------\n",
      "Precessing Row 395 ----------\n",
      "Precessing Row 396 ----------\n",
      "Precessing Row 397 ----------\n",
      "Precessing Row 398 ----------\n",
      "Precessing Row 399 ----------\n",
      "Precessing Row 400 ----------\n",
      "Precessing Row 401 ----------\n",
      "Precessing Row 402 ----------\n",
      "Precessing Row 403 ----------\n",
      "Precessing Row 404 ----------\n",
      "Precessing Row 405 ----------\n",
      "Precessing Row 406 ----------\n",
      "Precessing Row 407 ----------\n",
      "Precessing Row 408 ----------\n",
      "Precessing Row 409 ----------\n",
      "Precessing Row 410 ----------\n",
      "Precessing Row 411 ----------\n",
      "Precessing Row 412 ----------\n",
      "Precessing Row 413 ----------\n",
      "Precessing Row 414 ----------\n",
      "Precessing Row 415 ----------\n",
      "Precessing Row 416 ----------\n",
      "Precessing Row 417 ----------\n",
      "Precessing Row 418 ----------\n",
      "Precessing Row 419 ----------\n",
      "Precessing Row 420 ----------\n",
      "Precessing Row 421 ----------\n",
      "Precessing Row 422 ----------\n",
      "Precessing Row 423 ----------\n",
      "Precessing Row 424 ----------\n",
      "Precessing Row 425 ----------\n",
      "Precessing Row 426 ----------\n",
      "Precessing Row 427 ----------\n",
      "Precessing Row 428 ----------\n",
      "Precessing Row 429 ----------\n",
      "Precessing Row 430 ----------\n",
      "Precessing Row 431 ----------\n",
      "Precessing Row 432 ----------\n",
      "Precessing Row 433 ----------\n",
      "Precessing Row 434 ----------\n",
      "Precessing Row 435 ----------\n",
      "Precessing Row 436 ----------\n",
      "Precessing Row 437 ----------\n",
      "Precessing Row 438 ----------\n",
      "Precessing Row 439 ----------\n",
      "Precessing Row 440 ----------\n",
      "Precessing Row 441 ----------\n",
      "Precessing Row 442 ----------\n",
      "Precessing Row 443 ----------\n",
      "Precessing Row 444 ----------\n",
      "Precessing Row 445 ----------\n",
      "Precessing Row 446 ----------\n",
      "Precessing Row 447 ----------\n",
      "Precessing Row 448 ----------\n",
      "Precessing Row 449 ----------\n",
      "Precessing Row 450 ----------\n",
      "Precessing Row 451 ----------\n",
      "Precessing Row 452 ----------\n",
      "Precessing Row 453 ----------\n",
      "Precessing Row 454 ----------\n",
      "Precessing Row 455 ----------\n",
      "Precessing Row 456 ----------\n",
      "Precessing Row 457 ----------\n",
      "Precessing Row 458 ----------\n",
      "Precessing Row 459 ----------\n",
      "Precessing Row 460 ----------\n",
      "Precessing Row 461 ----------\n",
      "Precessing Row 462 ----------\n",
      "Precessing Row 463 ----------\n",
      "Precessing Row 464 ----------\n",
      "Precessing Row 465 ----------\n",
      "Precessing Row 466 ----------\n",
      "Precessing Row 467 ----------\n",
      "Precessing Row 468 ----------\n",
      "Precessing Row 469 ----------\n",
      "Precessing Row 470 ----------\n",
      "Precessing Row 471 ----------\n",
      "Precessing Row 472 ----------\n",
      "Precessing Row 473 ----------\n",
      "Precessing Row 474 ----------\n",
      "Precessing Row 475 ----------\n",
      "Precessing Row 476 ----------\n",
      "Precessing Row 477 ----------\n",
      "Precessing Row 478 ----------\n",
      "Precessing Row 479 ----------\n",
      "Precessing Row 480 ----------\n",
      "Precessing Row 481 ----------\n",
      "Precessing Row 482 ----------\n",
      "Precessing Row 483 ----------\n",
      "Precessing Row 484 ----------\n",
      "Precessing Row 485 ----------\n",
      "Precessing Row 486 ----------\n",
      "Precessing Row 487 ----------\n",
      "Precessing Row 488 ----------\n",
      "Precessing Row 489 ----------\n",
      "Precessing Row 490 ----------\n",
      "Precessing Row 491 ----------\n",
      "Precessing Row 492 ----------\n",
      "Precessing Row 493 ----------\n",
      "Precessing Row 494 ----------\n",
      "Precessing Row 495 ----------\n",
      "Precessing Row 496 ----------\n",
      "Precessing Row 497 ----------\n",
      "Precessing Row 498 ----------\n",
      "Precessing Row 499 ----------\n",
      "Detected PII entities saved to pii_entities_detected.txt\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "# Define the type alias for PII entity\n",
    "type pii_entity = Tuple[int, str, str, Tuple[int, int]]\n",
    "\n",
    "# Function to analyze text with Presidio and return PII entities\n",
    "def analyze_texts_with_presidio(df: pd.DataFrame) -> List[pii_entity]:\n",
    "    pii_entities: List[pii_entity] = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"Precessing Row {i} ----------\")\n",
    "        text = row.full_text\n",
    "        results_analyzed, results_anonymized = de_identify_pii(text)\n",
    "        \n",
    "        for result in results_analyzed:\n",
    "            start = result.start\n",
    "            end = result.end  # Presidio's end index is exclusive\n",
    "            entity_text = text[start:end]\n",
    "            pii_entities.append((i, entity_text, result.entity_type, (start, end)))\n",
    "    \n",
    "    return pii_entities\n",
    "\n",
    "# Function to save PII entities to a file\n",
    "def save_entities_to_file(entities: List[pii_entity], file_path: str, indent: int = 4):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for entity in entities:\n",
    "            indent_space = ' ' * indent\n",
    "            entity_str = f\"{entity}\\n\"\n",
    "            f.write(indent_space + entity_str)\n",
    "\n",
    "# Get PII entities for all rows in the dataframe\n",
    "pii_entities_detected = analyze_texts_with_presidio(df[:500])\n",
    "\n",
    "# Save the detected PII entities to a file\n",
    "output_file = \"pii_entities_detected.txt\"\n",
    "save_entities_to_file(pii_entities_detected, output_file)\n",
    "\n",
    "print(f\"Detected PII entities saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
